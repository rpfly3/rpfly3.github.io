{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pengfei's Docs","title":"Home"},{"location":"#welcome-to-pengfeis-docs","text":"","title":"Welcome to Pengfei's Docs"},{"location":"about/","text":"About Me","title":"About"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"av/papers/","text":"Papers Detection Image Classification 2021 ViT : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR 2021) [ notes ] 2020 RegNet : Designing Network Design Spaces (CVPR 2020) [notes] 2019 EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019) [notes] 2D Object Detection 2021 Deformable DETR: Deformable transformers for end-to-end object detection (ICLR 2021) [notes] 2020 EfficientDet: Scalable and Efficient Object Detection (CVPR 2020) [notes] DETR : End-to-End Object Detection with Transformers (ECCV 2020) [notes] 2012 AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ] Lane Detection 2022 PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark (ECCV 2022 Oral) [ notes ] 2018 LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ] Tracking SLAM 2020 A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ] Misc 2020 NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020 Oral) [notes]","title":"Papers"},{"location":"av/papers/#papers","text":"","title":"Papers"},{"location":"av/papers/#detection","text":"","title":"Detection"},{"location":"av/papers/#image-classification","text":"","title":"Image Classification"},{"location":"av/papers/#2021","text":"ViT : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR 2021) [ notes ]","title":"2021"},{"location":"av/papers/#2020","text":"RegNet : Designing Network Design Spaces (CVPR 2020) [notes]","title":"2020"},{"location":"av/papers/#2019","text":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019) [notes]","title":"2019"},{"location":"av/papers/#2d-object-detection","text":"","title":"2D Object Detection"},{"location":"av/papers/#2021_1","text":"Deformable DETR: Deformable transformers for end-to-end object detection (ICLR 2021) [notes]","title":"2021"},{"location":"av/papers/#2020_1","text":"EfficientDet: Scalable and Efficient Object Detection (CVPR 2020) [notes] DETR : End-to-End Object Detection with Transformers (ECCV 2020) [notes]","title":"2020"},{"location":"av/papers/#2012","text":"AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ]","title":"2012"},{"location":"av/papers/#lane-detection","text":"","title":"Lane Detection"},{"location":"av/papers/#2022","text":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark (ECCV 2022 Oral) [ notes ]","title":"2022"},{"location":"av/papers/#2018","text":"LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ]","title":"2018"},{"location":"av/papers/#tracking","text":"","title":"Tracking"},{"location":"av/papers/#slam","text":"","title":"SLAM"},{"location":"av/papers/#2020_2","text":"A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ]","title":"2020"},{"location":"av/papers/#misc","text":"","title":"Misc"},{"location":"av/papers/#2020_3","text":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020 Oral) [notes]","title":"2020"},{"location":"av/detection/","text":"Detection Modules Attention101 Transformer Vision Transformer","title":"Detection"},{"location":"av/detection/#detection","text":"","title":"Detection"},{"location":"av/detection/#modules","text":"Attention101 Transformer Vision Transformer","title":"Modules"},{"location":"av/detection/alex_net/alex_net/","text":"AlexNet Rich feature hierarchies for accurate object detection and semantic segmentation Abstract In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Introduction AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption. Object detection with R-CNN Module design Region proposals R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work. Feature extraction They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly \\( \\(p\\) \\) pixels of warped image context around the original box (they use \\( \\(p = 16\\) \\) ). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible. Test-time detection At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#alexnet","text":"","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation","text":"","title":"Rich feature hierarchies for accurate object detection and semantic segmentation"},{"location":"av/detection/alex_net/alex_net/#abstract","text":"In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.","title":"Abstract"},{"location":"av/detection/alex_net/alex_net/#introduction","text":"AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption.","title":"Introduction"},{"location":"av/detection/alex_net/alex_net/#object-detection-with-r-cnn","text":"","title":"Object detection with R-CNN"},{"location":"av/detection/alex_net/alex_net/#module-design","text":"","title":"Module design"},{"location":"av/detection/alex_net/alex_net/#region-proposals","text":"R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work.","title":"Region proposals"},{"location":"av/detection/alex_net/alex_net/#feature-extraction","text":"They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly \\( \\(p\\) \\) pixels of warped image context around the original box (they use \\( \\(p = 16\\) \\) ). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible.","title":"Feature extraction"},{"location":"av/detection/alex_net/alex_net/#test-time-detection","text":"At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"Test-time detection"},{"location":"av/detection/lane_net/lane_net/","text":"Towards End-to-End Lane Detection: an Instance Segmentation Approach Motivation Traditional Lane Detection Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations. Deep Learning Models More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances. Proposed Approach This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation. LaneNet Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances. H-Net \ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes. Method LaneNet The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network. Binary Segmentation \ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting. Instance Segmentation \ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized. Clustering The clustering is done by an iterative procedure. By setting \\(\\delta_d > 6\\delta_v\\) in the above loss, one can take a random lane embedding and threshold around it with a radius of \\(2 \\delta_v\\) to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding. Network Architecture \ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network Curve Fitting Using H-Net \ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial. Loss Function Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients. Network Architecture The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs. Results Dataset TuSimple lane dataset","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#towards-end-to-end-lane-detection-an-instance-segmentation-approach","text":"","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#motivation","text":"","title":"Motivation"},{"location":"av/detection/lane_net/lane_net/#traditional-lane-detection","text":"Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations.","title":"Traditional Lane Detection"},{"location":"av/detection/lane_net/lane_net/#deep-learning-models","text":"More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances.","title":"Deep Learning Models"},{"location":"av/detection/lane_net/lane_net/#proposed-approach","text":"This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation.","title":"Proposed Approach"},{"location":"av/detection/lane_net/lane_net/#lanenet","text":"Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#h-net","text":"\ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes.","title":"H-Net"},{"location":"av/detection/lane_net/lane_net/#method","text":"","title":"Method"},{"location":"av/detection/lane_net/lane_net/#lanenet_1","text":"The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#binary-segmentation","text":"\ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting.","title":"Binary Segmentation"},{"location":"av/detection/lane_net/lane_net/#instance-segmentation","text":"\ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized.","title":"Instance Segmentation"},{"location":"av/detection/lane_net/lane_net/#clustering","text":"The clustering is done by an iterative procedure. By setting \\(\\delta_d > 6\\delta_v\\) in the above loss, one can take a random lane embedding and threshold around it with a radius of \\(2 \\delta_v\\) to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding.","title":"Clustering"},{"location":"av/detection/lane_net/lane_net/#network-architecture","text":"\ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#curve-fitting-using-h-net","text":"\ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial.","title":"Curve Fitting Using H-Net"},{"location":"av/detection/lane_net/lane_net/#loss-function","text":"Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients.","title":"Loss Function"},{"location":"av/detection/lane_net/lane_net/#network-architecture_1","text":"The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs.","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#results","text":"","title":"Results"},{"location":"av/detection/lane_net/lane_net/#dataset","text":"TuSimple lane dataset","title":"Dataset"},{"location":"av/detection/modules/attention/attention/","text":"Attention 101 A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images, etc.) and outputs another sequence of items (see [ NLP ]). Under the hood, the model is composed of an encoder and a decoder. . The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item. Context The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks. Hint Look at the hidden states for the encoder, and notice how the last hidden state is actually the context we pass along to the decoder. The decoder also maintains a hidden state that it passes from one time step to the next. Attention The context vector turned to be a bottleneck fo these types of models. It made it challenging for the models to deal with long sequences. A solution was proposed in Neural Machine Translation by Jointly Learning to Align and Translate Effective Approaches to Attention-based Neural Machine Translation They introduced and refined a technique called Attention , which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed. At time step 7, the attention mechanism enables the decoder to focus on the word \"\u00e9tudiant\" (\"student\" in french) before it generates the English translation. Note The ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention. An attention model differs from a classic sequence-to-sequence model in both encoding and decoding. Encoding The encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder. Decoding An attention decoder does an extra step before producing its output. In order to focus on the parts of the input that relevant to this decoding time step, the decoder does the following: Look at the set of encoder hidden states it received -- each encoder hidden state is most associated with a certain word in the input sequence. Given each hidden state a score (ignore how the scoring is done for now). Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. The scoring exercise is done at each time step on the decoder side. The attention decoder RNN takes in the embedding of <END> token, and an initial decoder hidden state. The RNN processes its inputs, producing an output and a new hidden state vector ( h4 ). The output is discarded. Attention Step: we use the encoder hidden states and the h4 vector to calculate a context vector ( C4 ) for this time step. We concatenate h4 and C4 into one vector. We pass this vector through a feed forward neural network (one trained jointly with the model). The output of the feed forward neural networks indicates the output word of this time step. Repeat for the next time steps. Reference Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)","title":"Attention 101"},{"location":"av/detection/modules/attention/attention/#attention-101","text":"A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images, etc.) and outputs another sequence of items (see [ NLP ]). Under the hood, the model is composed of an encoder and a decoder. . The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.","title":"Attention 101"},{"location":"av/detection/modules/attention/attention/#context","text":"The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks. Hint Look at the hidden states for the encoder, and notice how the last hidden state is actually the context we pass along to the decoder. The decoder also maintains a hidden state that it passes from one time step to the next.","title":"Context"},{"location":"av/detection/modules/attention/attention/#attention","text":"The context vector turned to be a bottleneck fo these types of models. It made it challenging for the models to deal with long sequences. A solution was proposed in Neural Machine Translation by Jointly Learning to Align and Translate Effective Approaches to Attention-based Neural Machine Translation They introduced and refined a technique called Attention , which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed. At time step 7, the attention mechanism enables the decoder to focus on the word \"\u00e9tudiant\" (\"student\" in french) before it generates the English translation. Note The ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention. An attention model differs from a classic sequence-to-sequence model in both encoding and decoding.","title":"Attention"},{"location":"av/detection/modules/attention/attention/#encoding","text":"The encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.","title":"Encoding"},{"location":"av/detection/modules/attention/attention/#decoding","text":"An attention decoder does an extra step before producing its output. In order to focus on the parts of the input that relevant to this decoding time step, the decoder does the following: Look at the set of encoder hidden states it received -- each encoder hidden state is most associated with a certain word in the input sequence. Given each hidden state a score (ignore how the scoring is done for now). Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. The scoring exercise is done at each time step on the decoder side. The attention decoder RNN takes in the embedding of <END> token, and an initial decoder hidden state. The RNN processes its inputs, producing an output and a new hidden state vector ( h4 ). The output is discarded. Attention Step: we use the encoder hidden states and the h4 vector to calculate a context vector ( C4 ) for this time step. We concatenate h4 and C4 into one vector. We pass this vector through a feed forward neural network (one trained jointly with the model). The output of the feed forward neural networks indicates the output word of this time step. Repeat for the next time steps.","title":"Decoding"},{"location":"av/detection/modules/attention/attention/#reference","text":"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)","title":"Reference"},{"location":"av/detection/modules/nlp/nlp/","text":"Natural Language Processing with RNNs and Attention An Encoder-Decoder Network for Machine Translation Let's take a look at a simple neural machine translation model that will translate English sentences to French. In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note The French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it should have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that at inference time (after training), you will not have the target sentence to feed to the encoder. Instead, simply feed the decoder the word that it output at the previous step. Reference Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition","title":"Natural Language Processing with RNNs and Attention"},{"location":"av/detection/modules/nlp/nlp/#natural-language-processing-with-rnns-and-attention","text":"","title":"Natural Language Processing with RNNs and Attention"},{"location":"av/detection/modules/nlp/nlp/#an-encoder-decoder-network-for-machine-translation","text":"Let's take a look at a simple neural machine translation model that will translate English sentences to French. In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note The French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it should have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that at inference time (after training), you will not have the target sentence to feed to the encoder. Instead, simply feed the decoder the word that it output at the previous step.","title":"An Encoder-Decoder Network for Machine Translation"},{"location":"av/detection/modules/nlp/nlp/#reference","text":"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition","title":"Reference"},{"location":"av/detection/modules/rnn/rnn/","text":"Recurrent Neural Network Reference A friendly introduction to Recurrent Neural Networks","title":"Recurrent Neural Network"},{"location":"av/detection/modules/rnn/rnn/#recurrent-neural-network","text":"","title":"Recurrent Neural Network"},{"location":"av/detection/modules/rnn/rnn/#reference","text":"A friendly introduction to Recurrent Neural Networks","title":"Reference"},{"location":"av/detection/modules/transformer/transformer/","text":"Transformer A High-Level Look In a machine translation application, it would take a sentence in one language, and output its translation in another. Encoding The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2014 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number. The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers: Self-attention layer: a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Feed-forward network: the exact same feed-forward network is independently applied to each position. Decoding The decoder has both Self-attention layer and Feed Forward layer, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence. Encoding Bringing The Tensors Into The Picture The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512. The size of the list is a hyper-parameter we can set \u2014 basically it would be the length of the longest sentence in our training dataset. In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. Here we begin to see one key property of the Transformer: Note The word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. An encoder receives a list of vectors as input, and processes this list by passing these vectors into a self-attention layer, then into a feed-forward neural network ( the exact same network with each vector flowing through it separately ), then sends out the output upwards to the next encoder. Self-Attention Note Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing. As the model processes each word (each position in the input sequence), self-attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. Self-Attention in Detail First Step The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector Note These vectors are created by multiplying the embedding by three matrices that we trained during the training process. Notice that these new vectors are smaller in dimension than the embedding vector. They don\u2019t have to be smaller, this is an architecture choice to make the computation of multi-headed attention (mostly) constant. Second Step The second step in calculating self-attention is to calculate a score. Note The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. The score determines how much focus to place on other parts of the input sentence as we encode at a certain position. So if we\u2019re processing the self-attention for the word in position #1: the first score would be the dot product of q1 and k1. the second score would be the dot product of q1 and k2. Third and Fourth Steps The third step is to divide the scores by the square root of the dimension of the key vectors used in the paper. This leads to having more stable gradients. There could be other possible values here, but this is the default. In the fourth step, we pass the result through a softmax operation. Softmax normalizes the scores so they\u2019are all positive and add up to 1. This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word. Fifth and Sixth Steps The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers). The sixth steps is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position. That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. Matrix Calculation of Self-Attention The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X , and multiplying it by the weight matrices we've trained ( \\(W^Q\\) , \\(W^K\\) , \\(W^V\\) ). The other steps can be condensed into this formula: Multi-Headed Attention Multi-headed attention improves the performance of the attention layer in two ways: It expands the model\u2019s ability to focus on different positions. In the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. It gives the attention layer multiple \u201crepresentation subspaces\u201d. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder) . Each of these is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices. This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2014 it\u2019s expecting a single matrix (a vector for each word). We need a way to condense these eight down into a single matrix. We concatenate the matrices then multiply them by an additional weights matrix \\(W^O\\) . Note The result would be the Z matrix that captures information from all the attention heads. We can send this matrix forward to the feed forward layers. Put everything together: Representing The Order of The Sequence Using Positional Encoding One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. Note The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vector once they\u2019re projected into Q/K/V vectors and during dot-product attention. There are some different possible methods for positional encoding. The Residuals One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer in each encoder has a residual connection around it, and is followed by a layer-normalization step. This goes for the sub-layers of the decoder as well. Decoding The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V . These are to be used by decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence. The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. The self-attention layers in the decoder operate in a slightly different way than the one in the encoder. Note In the decoder, the self-attention layer is only allowed to attend the earlier positions in the output sequence. This is done by masking future positions (setting them to -inf ) before the softmax step in the self-attention calculation. Encoder-Decoder Attention The \u201cEncoder-Decoder Attention\u201d layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack. The Final Linear and Softmax Layer The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let\u2019s assume that our model knows 10000 unique English words that it\u2019s learned from its training dataset. This would make the logits vector 10000 cells wide \u2014 each cell corresponding to the scope of a unique word. That is how we interpret the output of the model followed by the Linear layer. Reference Attention Is All You Need The Illustrated Transformer The Narrated Transformer Language Model The Annotated Transformer","title":"Transformer"},{"location":"av/detection/modules/transformer/transformer/#transformer","text":"","title":"Transformer"},{"location":"av/detection/modules/transformer/transformer/#a-high-level-look","text":"In a machine translation application, it would take a sentence in one language, and output its translation in another.","title":"A High-Level Look"},{"location":"av/detection/modules/transformer/transformer/#encoding","text":"The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2014 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number. The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers: Self-attention layer: a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Feed-forward network: the exact same feed-forward network is independently applied to each position.","title":"Encoding"},{"location":"av/detection/modules/transformer/transformer/#decoding","text":"The decoder has both Self-attention layer and Feed Forward layer, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.","title":"Decoding"},{"location":"av/detection/modules/transformer/transformer/#encoding_1","text":"","title":"Encoding"},{"location":"av/detection/modules/transformer/transformer/#bringing-the-tensors-into-the-picture","text":"The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512. The size of the list is a hyper-parameter we can set \u2014 basically it would be the length of the longest sentence in our training dataset. In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. Here we begin to see one key property of the Transformer: Note The word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. An encoder receives a list of vectors as input, and processes this list by passing these vectors into a self-attention layer, then into a feed-forward neural network ( the exact same network with each vector flowing through it separately ), then sends out the output upwards to the next encoder.","title":"Bringing The Tensors Into The Picture"},{"location":"av/detection/modules/transformer/transformer/#self-attention","text":"Note Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing. As the model processes each word (each position in the input sequence), self-attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.","title":"Self-Attention"},{"location":"av/detection/modules/transformer/transformer/#self-attention-in-detail","text":"","title":"Self-Attention in Detail"},{"location":"av/detection/modules/transformer/transformer/#first-step","text":"The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector Note These vectors are created by multiplying the embedding by three matrices that we trained during the training process. Notice that these new vectors are smaller in dimension than the embedding vector. They don\u2019t have to be smaller, this is an architecture choice to make the computation of multi-headed attention (mostly) constant.","title":"First Step"},{"location":"av/detection/modules/transformer/transformer/#second-step","text":"The second step in calculating self-attention is to calculate a score. Note The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. The score determines how much focus to place on other parts of the input sentence as we encode at a certain position. So if we\u2019re processing the self-attention for the word in position #1: the first score would be the dot product of q1 and k1. the second score would be the dot product of q1 and k2.","title":"Second Step"},{"location":"av/detection/modules/transformer/transformer/#third-and-fourth-steps","text":"The third step is to divide the scores by the square root of the dimension of the key vectors used in the paper. This leads to having more stable gradients. There could be other possible values here, but this is the default. In the fourth step, we pass the result through a softmax operation. Softmax normalizes the scores so they\u2019are all positive and add up to 1. This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word.","title":"Third and Fourth Steps"},{"location":"av/detection/modules/transformer/transformer/#fifth-and-sixth-steps","text":"The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers). The sixth steps is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position. That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network.","title":"Fifth and Sixth Steps"},{"location":"av/detection/modules/transformer/transformer/#matrix-calculation-of-self-attention","text":"The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X , and multiplying it by the weight matrices we've trained ( \\(W^Q\\) , \\(W^K\\) , \\(W^V\\) ). The other steps can be condensed into this formula:","title":"Matrix Calculation of Self-Attention"},{"location":"av/detection/modules/transformer/transformer/#multi-headed-attention","text":"Multi-headed attention improves the performance of the attention layer in two ways: It expands the model\u2019s ability to focus on different positions. In the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. It gives the attention layer multiple \u201crepresentation subspaces\u201d. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder) . Each of these is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices. This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2014 it\u2019s expecting a single matrix (a vector for each word). We need a way to condense these eight down into a single matrix. We concatenate the matrices then multiply them by an additional weights matrix \\(W^O\\) . Note The result would be the Z matrix that captures information from all the attention heads. We can send this matrix forward to the feed forward layers. Put everything together:","title":"Multi-Headed Attention"},{"location":"av/detection/modules/transformer/transformer/#representing-the-order-of-the-sequence-using-positional-encoding","text":"One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. Note The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vector once they\u2019re projected into Q/K/V vectors and during dot-product attention. There are some different possible methods for positional encoding.","title":"Representing The Order of The Sequence Using Positional Encoding"},{"location":"av/detection/modules/transformer/transformer/#the-residuals","text":"One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer in each encoder has a residual connection around it, and is followed by a layer-normalization step. This goes for the sub-layers of the decoder as well.","title":"The Residuals"},{"location":"av/detection/modules/transformer/transformer/#decoding_1","text":"The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V . These are to be used by decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence. The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. The self-attention layers in the decoder operate in a slightly different way than the one in the encoder. Note In the decoder, the self-attention layer is only allowed to attend the earlier positions in the output sequence. This is done by masking future positions (setting them to -inf ) before the softmax step in the self-attention calculation.","title":"Decoding"},{"location":"av/detection/modules/transformer/transformer/#encoder-decoder-attention","text":"The \u201cEncoder-Decoder Attention\u201d layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.","title":"Encoder-Decoder Attention"},{"location":"av/detection/modules/transformer/transformer/#the-final-linear-and-softmax-layer","text":"The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let\u2019s assume that our model knows 10000 unique English words that it\u2019s learned from its training dataset. This would make the logits vector 10000 cells wide \u2014 each cell corresponding to the scope of a unique word. That is how we interpret the output of the model followed by the Linear layer.","title":"The Final Linear and Softmax Layer"},{"location":"av/detection/modules/transformer/transformer/#reference","text":"Attention Is All You Need The Illustrated Transformer The Narrated Transformer Language Model The Annotated Transformer","title":"Reference"},{"location":"av/detection/modules/vision_transformer/vit/","text":"Vision Transformer","title":"Vision Transformer"},{"location":"av/detection/modules/vision_transformer/vit/#vision-transformer","text":"","title":"Vision Transformer"},{"location":"av/detection/persformer/notes/","text":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark Abstract Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. OpenLane: one of the first large-scale real-world 3D lane datasets, with high-quality annotation and scenario diversity. Introduction 2D Lane Detection With the prosperity of deep learning, lane detection algorithms in the 2D image space has achieved impressive results, where the task is formulated as a 2D segmentation problem given front view (perspective) image as input. Warning Such a framework to perform lane detection in the perspective view is not applicable for industry-level products where complicated scenarios dominate. BEV Perception Downstream modules as in planning and control often require the lane location to be in the form of the orthographic bird's eye view (BEV) instead of a front view representation. Representation in BEV is for better task alignment with interactive agents (vehicle, road marker, traffic light, etc.) in the environment and multi-modal compatibility with other sensors such as LiDAR and Radar. The conventional approaches to address such a demand are: either to simply project perspective lanes to ones in the BEV space, or more elegantly to cast perspective features to BEV by aid of camera in/extrinsic matrices. Spatial Transformer The latter solution is inspired by the spatial transformer network (STN) to generate a one-to-one correspondence from the image to BEV feature grids. By doing so, the quality of features in BEV depends solely on the quality of the corresponding feature in the front view. Warning The predictions using these outcome features are not adorable as the blemish of scale variance in the front view, which inherits from the camera's pinhole model, remains. Lane Line Height The height of lane lines has to be considered when we project perspective lanes into BEV space. The lanes would diverge/converge in case of uphill/downhill if the height is ignored, if the height is ignored, leading to improper action decisions as in the planning and control module. Previous literature inevitably hypothesize that lanes in the BEV space lie on a flat ground, i.e., the height of lanes is zero. The planar assumption does not hold true in most autonomous driving scenarios, e.g., uphill/downhill, bump, crush turn, etc. Failure Since the height information is unavailable on public benchmarks or complicated to acquire accurate ground truth, 3D lane detection is ill-posed. PersFormer A unified 2D/3D lane detection framework with Transformer. Transformer-based Spatial Feature Transformation Module They model the spatial feature transformation as a learning procedure that has an attention mechanism to capture the interaction both among local region in the front view feature and between two views (front view to BEV), consequently being able to generate a fine-grained BEV feature representation. the deformable attention mechanism is adopted to remarkably reduce the computational memory requirement. dynamically adjust keys through the cross-attention module to capture prominent feature among the local region. Unified 2D and 3D Lane Detection Tasks They further unify 2D and 3D lane detection tasks to benefit from the co-learning optimization. Related Work Vision Transformers in Bird's-Eye-View (BEV) Note Projecting features to BEV and performing downstream tasks in it has become more dominant and ensured better performance recently. Compared with conventional CNN structure, the cross attention scheme in Vision Transformers is naturally introduced to serve as a learnable transformation of features across different views. Instead of simply projecting features via IPM, the successful application of Transformers in view transformation has demonstrated great success in various domains, including 3D object detection, prediction, planning, etc. 3D Lane Detection 3D-LaneNet: End-to-End 3D Multiple Lane Detection (ICCV 2019) Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection (ECCV 2020) Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021) Methodology 2D/3D lane attributes: ordered set of coordinates categorical attribute indicating the type of lane each point has its visibility attribute Approach Overview The backbone takes the resized image as input and generates multi-scale front view features where EfficientNet is adopted. The Perspective Transformer takes the front view features as input and generates BEV features by the aid of camera intrinsic and extrinsic parameters. The lane detection heads are responsible for predicting 2D/3D coordinates as well as lane types. The 2D/3D detection heads are referred to as LaneATT and 3D-LaneNet with modification on the structure and anchor design. Proposed Perspective Transformer Note The general idea of Perspective Transformer is to use the coordinates transformation matrix from IPM as a reference to generate BEV feature representation, by attending related region (local context) in front view feature. On the assumption that the ground is flat and the camera parameters are given, a classical IPM approach calculates a set of coordinate mapping from front-view to BEV, where the BEV space is defined on the flat ground. Such a transformation enframes a strong prior on the attention unit in PersFormer to generate more representative BEV features. Tip The architecture of Perspective Transformer is inspired by popular approaches such as DETR , and consists of the self-attention module and cross-attention module.","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark"},{"location":"av/detection/persformer/notes/#persformer-3d-lane-detection-via-perspective-transformer-and-the-openlane-benchmark","text":"Abstract Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. OpenLane: one of the first large-scale real-world 3D lane datasets, with high-quality annotation and scenario diversity.","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark"},{"location":"av/detection/persformer/notes/#introduction","text":"","title":"Introduction"},{"location":"av/detection/persformer/notes/#2d-lane-detection","text":"With the prosperity of deep learning, lane detection algorithms in the 2D image space has achieved impressive results, where the task is formulated as a 2D segmentation problem given front view (perspective) image as input. Warning Such a framework to perform lane detection in the perspective view is not applicable for industry-level products where complicated scenarios dominate.","title":"2D Lane Detection"},{"location":"av/detection/persformer/notes/#bev-perception","text":"Downstream modules as in planning and control often require the lane location to be in the form of the orthographic bird's eye view (BEV) instead of a front view representation. Representation in BEV is for better task alignment with interactive agents (vehicle, road marker, traffic light, etc.) in the environment and multi-modal compatibility with other sensors such as LiDAR and Radar. The conventional approaches to address such a demand are: either to simply project perspective lanes to ones in the BEV space, or more elegantly to cast perspective features to BEV by aid of camera in/extrinsic matrices.","title":"BEV Perception"},{"location":"av/detection/persformer/notes/#spatial-transformer","text":"The latter solution is inspired by the spatial transformer network (STN) to generate a one-to-one correspondence from the image to BEV feature grids. By doing so, the quality of features in BEV depends solely on the quality of the corresponding feature in the front view. Warning The predictions using these outcome features are not adorable as the blemish of scale variance in the front view, which inherits from the camera's pinhole model, remains.","title":"Spatial Transformer"},{"location":"av/detection/persformer/notes/#lane-line-height","text":"The height of lane lines has to be considered when we project perspective lanes into BEV space. The lanes would diverge/converge in case of uphill/downhill if the height is ignored, if the height is ignored, leading to improper action decisions as in the planning and control module. Previous literature inevitably hypothesize that lanes in the BEV space lie on a flat ground, i.e., the height of lanes is zero. The planar assumption does not hold true in most autonomous driving scenarios, e.g., uphill/downhill, bump, crush turn, etc. Failure Since the height information is unavailable on public benchmarks or complicated to acquire accurate ground truth, 3D lane detection is ill-posed.","title":"Lane Line Height"},{"location":"av/detection/persformer/notes/#persformer","text":"A unified 2D/3D lane detection framework with Transformer.","title":"PersFormer"},{"location":"av/detection/persformer/notes/#transformer-based-spatial-feature-transformation-module","text":"They model the spatial feature transformation as a learning procedure that has an attention mechanism to capture the interaction both among local region in the front view feature and between two views (front view to BEV), consequently being able to generate a fine-grained BEV feature representation. the deformable attention mechanism is adopted to remarkably reduce the computational memory requirement. dynamically adjust keys through the cross-attention module to capture prominent feature among the local region.","title":"Transformer-based Spatial Feature Transformation Module"},{"location":"av/detection/persformer/notes/#unified-2d-and-3d-lane-detection-tasks","text":"They further unify 2D and 3D lane detection tasks to benefit from the co-learning optimization.","title":"Unified 2D and 3D Lane Detection Tasks"},{"location":"av/detection/persformer/notes/#related-work","text":"","title":"Related Work"},{"location":"av/detection/persformer/notes/#vision-transformers-in-birds-eye-view-bev","text":"Note Projecting features to BEV and performing downstream tasks in it has become more dominant and ensured better performance recently. Compared with conventional CNN structure, the cross attention scheme in Vision Transformers is naturally introduced to serve as a learnable transformation of features across different views. Instead of simply projecting features via IPM, the successful application of Transformers in view transformation has demonstrated great success in various domains, including 3D object detection, prediction, planning, etc.","title":"Vision Transformers in Bird's-Eye-View (BEV)"},{"location":"av/detection/persformer/notes/#3d-lane-detection","text":"3D-LaneNet: End-to-End 3D Multiple Lane Detection (ICCV 2019) Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection (ECCV 2020) Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021)","title":"3D Lane Detection"},{"location":"av/detection/persformer/notes/#methodology","text":"2D/3D lane attributes: ordered set of coordinates categorical attribute indicating the type of lane each point has its visibility attribute","title":"Methodology"},{"location":"av/detection/persformer/notes/#approach-overview","text":"The backbone takes the resized image as input and generates multi-scale front view features where EfficientNet is adopted. The Perspective Transformer takes the front view features as input and generates BEV features by the aid of camera intrinsic and extrinsic parameters. The lane detection heads are responsible for predicting 2D/3D coordinates as well as lane types. The 2D/3D detection heads are referred to as LaneATT and 3D-LaneNet with modification on the structure and anchor design.","title":"Approach Overview"},{"location":"av/detection/persformer/notes/#proposed-perspective-transformer","text":"Note The general idea of Perspective Transformer is to use the coordinates transformation matrix from IPM as a reference to generate BEV feature representation, by attending related region (local context) in front view feature. On the assumption that the ground is flat and the camera parameters are given, a classical IPM approach calculates a set of coordinate mapping from front-view to BEV, where the BEV space is defined on the flat ground. Such a transformation enframes a strong prior on the attention unit in PersFormer to generate more representative BEV features. Tip The architecture of Perspective Transformer is inspired by popular approaches such as DETR , and consists of the self-attention module and cross-attention module.","title":"Proposed Perspective Transformer"},{"location":"av/detection/vision_transformer/vit/","text":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Abstract In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. They show that the reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Vision Transformer (ViT) can attain excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. Introduction Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest pssible modifications. Note They split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. Related Work Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Several approximations have been tried: Apply the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions. Sparse Transformers employ scalable approximations to global self attention in order to be applicable to images. Apply self attention in blocks of varying sizes. ... Warning Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. A very similar work is On the Relationship between Self-Attention and Convolutional Layers (ICLR 2020) . The main differences are this work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. this work uses a larger patch size, which makes the model applicable to medium-resolution images as well. Method ViT follow the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures - and their efficient implementations - can be used almost out of the box. Vision Transformer (ViT) The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection.","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"location":"av/detection/vision_transformer/vit/#an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale","text":"Abstract In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. They show that the reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Vision Transformer (ViT) can attain excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"location":"av/detection/vision_transformer/vit/#introduction","text":"Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest pssible modifications. Note They split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application.","title":"Introduction"},{"location":"av/detection/vision_transformer/vit/#related-work","text":"Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Several approximations have been tried: Apply the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions. Sparse Transformers employ scalable approximations to global self attention in order to be applicable to images. Apply self attention in blocks of varying sizes. ... Warning Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. A very similar work is On the Relationship between Self-Attention and Convolutional Layers (ICLR 2020) . The main differences are this work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. this work uses a larger patch size, which makes the model applicable to medium-resolution images as well.","title":"Related Work"},{"location":"av/detection/vision_transformer/vit/#method","text":"ViT follow the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures - and their efficient implementations - can be used almost out of the box.","title":"Method"},{"location":"av/detection/vision_transformer/vit/#vision-transformer-vit","text":"The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection.","title":"Vision Transformer (ViT)"},{"location":"av/slam/","text":"SLAM","title":"SLAM"},{"location":"av/slam/#slam","text":"","title":"SLAM"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/","text":"A Survey on Deep Learning for Localization and Mapping Abstract \ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). Introduction Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI. Why to Study Deep Learning for Localization and Mapping The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power. Taxonomy of Existing Approaches We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems. Odometry Estimation We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale. Visual Odometry Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors. Supervised Learning of VO We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters \\(\\boldsymbol{\\theta}^*\\) of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations \\(\\hat{\\textbf{p}} \\in \\mathbb{R}^3\\) and euler angle based rotations \\(\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3\\) : DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric. Unsupervised Learning of VO This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image \\(\\textbf{I}_s\\) , the view synthesis task is to generate a synthetic target image \\(\\textbf{I}_t\\) . A pixel of source image \\(\\textbf{I}_s(p_s)\\) is projected onto a target view \\(\\textbf{I}_t(p_t)\\) via: where \\(\\textbf{K}\\) is the camera\u2019s intrinsic matrix, \\(\\textbf{T}_{t\u2192s}\\) denotes the camera motion matrix from target frame to source frame, and \\(\\textbf{D}_t(p_t)\\) denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where \\(p\\) denotes pixel coordinates, \\(\\textbf{I}_t\\) is the target image, and \\(\\hat{\\textbf{I}}_s\\) is the synthetic target image generated from the source image \\(\\textbf{I}_s\\) . However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions. Hybrid VO \ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#a-survey-on-deep-learning-for-localization-and-mapping","text":"","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#abstract","text":"\ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS).","title":"Abstract"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#introduction","text":"Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI.","title":"Introduction"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#why-to-study-deep-learning-for-localization-and-mapping","text":"The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power.","title":"Why to Study Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#taxonomy-of-existing-approaches","text":"We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems.","title":"Taxonomy of Existing Approaches"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#odometry-estimation","text":"We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale.","title":"Odometry Estimation"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#visual-odometry","text":"Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors.","title":"Visual Odometry"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#supervised-learning-of-vo","text":"We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters \\(\\boldsymbol{\\theta}^*\\) of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations \\(\\hat{\\textbf{p}} \\in \\mathbb{R}^3\\) and euler angle based rotations \\(\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3\\) : DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric.","title":"Supervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#unsupervised-learning-of-vo","text":"This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image \\(\\textbf{I}_s\\) , the view synthesis task is to generate a synthetic target image \\(\\textbf{I}_t\\) . A pixel of source image \\(\\textbf{I}_s(p_s)\\) is projected onto a target view \\(\\textbf{I}_t(p_t)\\) via: where \\(\\textbf{K}\\) is the camera\u2019s intrinsic matrix, \\(\\textbf{T}_{t\u2192s}\\) denotes the camera motion matrix from target frame to source frame, and \\(\\textbf{D}_t(p_t)\\) denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where \\(p\\) denotes pixel coordinates, \\(\\textbf{I}_t\\) is the target image, and \\(\\hat{\\textbf{I}}_s\\) is the synthetic target image generated from the source image \\(\\textbf{I}_s\\) . However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions.","title":"Unsupervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#hybrid-vo","text":"\ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"Hybrid VO"},{"location":"av/tracking/","text":"Tracking","title":"Tracking"},{"location":"av/tracking/#tracking","text":"","title":"Tracking"},{"location":"programming/cpp/","text":"C++","title":"C++"},{"location":"programming/cpp/#c","text":"","title":"C++"},{"location":"programming/cuda/","text":"CUDA","title":"CUDA"},{"location":"programming/cuda/#cuda","text":"","title":"CUDA"},{"location":"programming/python/","text":"Python","title":"Python"},{"location":"programming/python/#python","text":"","title":"Python"},{"location":"system/","text":"System","title":"System"},{"location":"system/#system","text":"","title":"System"},{"location":"system/bazel/","text":"","title":"bazel"},{"location":"system/docker/","text":"Docker","title":"docker"},{"location":"system/docker/#docker","text":"","title":"Docker"},{"location":"system/pytorch/","text":"PyTorch","title":"pytorch"},{"location":"system/pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"system/ros2/","text":"ROS2","title":"ros2"},{"location":"system/ros2/#ros2","text":"","title":"ROS2"}]}
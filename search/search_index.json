{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pengfei's Docs","title":"Home"},{"location":"#welcome-to-pengfeis-docs","text":"","title":"Welcome to Pengfei's Docs"},{"location":"about/","text":"About Me","title":"About"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"av/","text":"Detection 2D Object Detection 2012 AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ] Lane Detection 2018 LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ] Tracking SLAM 2020 A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ]","title":"Autonomous Vehicle"},{"location":"av/#detection","text":"","title":"Detection"},{"location":"av/#2d-object-detection","text":"","title":"2D Object Detection"},{"location":"av/#2012","text":"AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ]","title":"2012"},{"location":"av/#lane-detection","text":"","title":"Lane Detection"},{"location":"av/#2018","text":"LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ]","title":"2018"},{"location":"av/#tracking","text":"","title":"Tracking"},{"location":"av/#slam","text":"","title":"SLAM"},{"location":"av/#2020","text":"A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ]","title":"2020"},{"location":"av/detection/alex_net/alex_net/","text":"AlexNet Rich feature hierarchies for accurate object detection and semantic segmentation Abstract In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Introduction AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption. Object detection with R-CNN Module design Region proposals R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work. Feature extraction They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly $$p$$ pixels of warped image context around the original box (they use $$p = 16$$). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible. Test-time detection At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#alexnet","text":"","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation","text":"","title":"Rich feature hierarchies for accurate object detection and semantic segmentation"},{"location":"av/detection/alex_net/alex_net/#abstract","text":"In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.","title":"Abstract"},{"location":"av/detection/alex_net/alex_net/#introduction","text":"AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption.","title":"Introduction"},{"location":"av/detection/alex_net/alex_net/#object-detection-with-r-cnn","text":"","title":"Object detection with R-CNN"},{"location":"av/detection/alex_net/alex_net/#module-design","text":"","title":"Module design"},{"location":"av/detection/alex_net/alex_net/#region-proposals","text":"R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work.","title":"Region proposals"},{"location":"av/detection/alex_net/alex_net/#feature-extraction","text":"They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly $$p$$ pixels of warped image context around the original box (they use $$p = 16$$). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible.","title":"Feature extraction"},{"location":"av/detection/alex_net/alex_net/#test-time-detection","text":"At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"Test-time detection"},{"location":"av/detection/lane_net/lane_net/","text":"Towards End-to-End Lane Detection: an Instance Segmentation Approach Motivation Traditional Lane Detection Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations. Deep Learning Models More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances. Proposed Approach This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation. LaneNet Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances. H-Net \ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes. Method LaneNet The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network. Binary Segmentation \ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting. Instance Segmentation \ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized. Clustering The clustering is done by an iterative procedure. By setting $\\delta_d > 6\\delta_v$ in the above loss, one can take a random lane embedding and threshold around it with a radius of $2 \\delta_v$ to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding. Network Architecture \ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network Curve Fitting Using H-Net \ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial. Loss Function Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients. Network Architecture The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs. Results Dataset TuSimple lane dataset","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#towards-end-to-end-lane-detection-an-instance-segmentation-approach","text":"","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#motivation","text":"","title":"Motivation"},{"location":"av/detection/lane_net/lane_net/#traditional-lane-detection","text":"Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations.","title":"Traditional Lane Detection"},{"location":"av/detection/lane_net/lane_net/#deep-learning-models","text":"More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances.","title":"Deep Learning Models"},{"location":"av/detection/lane_net/lane_net/#proposed-approach","text":"This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation.","title":"Proposed Approach"},{"location":"av/detection/lane_net/lane_net/#lanenet","text":"Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#h-net","text":"\ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes.","title":"H-Net"},{"location":"av/detection/lane_net/lane_net/#method","text":"","title":"Method"},{"location":"av/detection/lane_net/lane_net/#lanenet_1","text":"The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#binary-segmentation","text":"\ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting.","title":"Binary Segmentation"},{"location":"av/detection/lane_net/lane_net/#instance-segmentation","text":"\ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized.","title":"Instance Segmentation"},{"location":"av/detection/lane_net/lane_net/#clustering","text":"The clustering is done by an iterative procedure. By setting $\\delta_d > 6\\delta_v$ in the above loss, one can take a random lane embedding and threshold around it with a radius of $2 \\delta_v$ to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding.","title":"Clustering"},{"location":"av/detection/lane_net/lane_net/#network-architecture","text":"\ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#curve-fitting-using-h-net","text":"\ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial.","title":"Curve Fitting Using H-Net"},{"location":"av/detection/lane_net/lane_net/#loss-function","text":"Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients.","title":"Loss Function"},{"location":"av/detection/lane_net/lane_net/#network-architecture_1","text":"The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs.","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#results","text":"","title":"Results"},{"location":"av/detection/lane_net/lane_net/#dataset","text":"TuSimple lane dataset","title":"Dataset"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/","text":"A Survey on Deep Learning for Localization and Mapping Abstract \ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). Introduction Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI. Why to Study Deep Learning for Localization and Mapping The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power. Taxonomy of Existing Approaches We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems. Odometry Estimation We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale. Visual Odometry Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors. Supervised Learning of VO We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters $\\boldsymbol{\\theta}^*$ of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations $\\hat{\\textbf{p}} \\in \\mathbb{R}^3$ and euler angle based rotations $\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3$: DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric. Unsupervised Learning of VO This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image $\\textbf{I}_s$, the view synthesis task is to generate a synthetic target image $\\textbf{I}_t$. A pixel of source image $\\textbf{I}_s(p_s)$ is projected onto a target view $\\textbf{I}_t(p_t)$ via: where $\\textbf{K}$ is the camera\u2019s intrinsic matrix, $\\textbf{T}_{t\u2192s}$ denotes the camera motion matrix from target frame to source frame, and $\\textbf{D}_t(p_t)$ denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where $p$ denotes pixel coordinates, $\\textbf{I}_t$ is the target image, and $\\hat{\\textbf{I}}_s$ is the synthetic target image generated from the source image $\\textbf{I}_s$. However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions. Hybrid VO \ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#a-survey-on-deep-learning-for-localization-and-mapping","text":"","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#abstract","text":"\ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS).","title":"Abstract"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#introduction","text":"Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI.","title":"Introduction"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#why-to-study-deep-learning-for-localization-and-mapping","text":"The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power.","title":"Why to Study Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#taxonomy-of-existing-approaches","text":"We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems.","title":"Taxonomy of Existing Approaches"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#odometry-estimation","text":"We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale.","title":"Odometry Estimation"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#visual-odometry","text":"Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors.","title":"Visual Odometry"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#supervised-learning-of-vo","text":"We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters $\\boldsymbol{\\theta}^*$ of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations $\\hat{\\textbf{p}} \\in \\mathbb{R}^3$ and euler angle based rotations $\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3$: DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric.","title":"Supervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#unsupervised-learning-of-vo","text":"This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image $\\textbf{I}_s$, the view synthesis task is to generate a synthetic target image $\\textbf{I}_t$. A pixel of source image $\\textbf{I}_s(p_s)$ is projected onto a target view $\\textbf{I}_t(p_t)$ via: where $\\textbf{K}$ is the camera\u2019s intrinsic matrix, $\\textbf{T}_{t\u2192s}$ denotes the camera motion matrix from target frame to source frame, and $\\textbf{D}_t(p_t)$ denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where $p$ denotes pixel coordinates, $\\textbf{I}_t$ is the target image, and $\\hat{\\textbf{I}}_s$ is the synthetic target image generated from the source image $\\textbf{I}_s$. However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions.","title":"Unsupervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#hybrid-vo","text":"\ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"Hybrid VO"},{"location":"cpp/","text":"C++","title":"C++"},{"location":"cpp/#c","text":"","title":"C++"},{"location":"system/","text":"System","title":"System"},{"location":"system/#system","text":"","title":"System"}]}
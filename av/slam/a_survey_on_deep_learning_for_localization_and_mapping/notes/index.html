
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.1, mkdocs-material-8.5.7">
    
    
      
        <title>A Survey on Deep Learning for Localization and Mapping - Pengfei's Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#a-survey-on-deep-learning-for-localization-and-mapping" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Pengfei&#39;s Docs" class="md-header__button md-logo" aria-label="Pengfei's Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.5 9V1.5h-3V9h-3V1.5h-3V9h-3V1.5H4.65V9H3v1.5h18V9h-1.5m0 4.5h-3V21h-3v-7.5h-3V21h-3v-7.5H4.65V21H3v1.5h18V21h-1.5v-7.5Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Pengfei's Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              A Survey on Deep Learning for Localization and Mapping
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../../cpp/" class="md-tabs__link">
        Documents
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../../about/" class="md-tabs__link">
      About
    </a>
  </li>

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Pengfei&#39;s Docs" class="md-nav__button md-logo" aria-label="Pengfei's Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19.5 9V1.5h-3V9h-3V1.5h-3V9h-3V1.5H4.65V9H3v1.5h18V9h-1.5m0 4.5h-3V21h-3v-7.5h-3V21h-3v-7.5H4.65V21H3v1.5h18V21h-1.5v-7.5Z"/></svg>

    </a>
    Pengfei's Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Documents
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Documents" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Documents
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../cpp/" class="md-nav__link">
        C++
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../" class="md-nav__link">
        Autonomous Vehicle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../system/" class="md-nav__link">
        System
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="a-survey-on-deep-learning-for-localization-and-mapping">A Survey on Deep Learning for Localization and Mapping</h1>
<h1 id="abstract">Abstract</h1>
<aside>
ðŸ’¡ Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way.

</aside>

<p>In this work,</p>
<ul>
<li>we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning.</li>
<li>we also discuss the limitations of current models, and indicate possible future directions.</li>
</ul>
<p>A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM).</p>
<aside>
ðŸ’¡ We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS).

</aside>

<h1 id="introduction">Introduction</h1>
<p>Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios.</p>
<blockquote>
<p>Such a quest is termed as â€˜Spatial Machine Intelligence System (SMIS)â€™ in our work or recently as Spatial AI.
</p>
</blockquote>
<h2 id="why-to-study-deep-learning-for-localization-and-mapping">Why to Study Deep Learning for Localization and Mapping</h2>
<p>The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM).</p>
<aside>
ðŸ’¡ In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems.

</aside>

<p><img alt="Screen Shot 2022-02-07 at 1.33.15 PM.png" src="../img/Screen_Shot_2022-02-07_at_1.33.15_PM.png" /></p>
<p>The advantages of learning based methods are three-fold:</p>
<ul>
<li>First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task.</li>
<li>Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information.</li>
<li>The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power.</li>
</ul>
<h1 id="taxonomy-of-existing-approaches">Taxonomy of Existing Approaches</h1>
<p>We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2.</p>
<p><img alt="Screen Shot 2022-02-07 at 1.50.21 PM.png" src="../img/Screen_Shot_2022-02-07_at_1.50.21_PM.png" /></p>
<p><strong>Odometry Estimation</strong></p>
<aside>
ðŸ’¡ Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data.

</aside>

<p>It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution.</p>
<aside>
ðŸ’¡ The key problem is to accurately estimate motion transformations from various sensor measurements.

</aside>

<p>To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way.</p>
<p><strong>Mapping</strong></p>
<aside>
ðŸ’¡ Mapping builds and reconstructs a consistent model to describe the surrounding environment.

</aside>

<p>Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. </p>
<p>Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. </p>
<p><strong>Global Localization</strong></p>
<aside>
ðŸ’¡ Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge.

</aside>

<p>This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before.</p>
<aside>
ðŸ’¡ It can be leveraged to reduce the pose drift of a dead reckoning system or solve the â€˜kidnapped robotâ€™ problem.

</aside>

<p>Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map.</p>
<p><strong>Simultaneous Localization and Mapping (SLAM)</strong></p>
<aside>
ðŸ’¡ SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping.

</aside>

<p>Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows:</p>
<ul>
<li>local optimization ensures the local consistency of camera motion and scene geometry;</li>
<li>global optimization aims to constrain the drift of global trajectories, and in a global scale;</li>
<li>keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection;</li>
<li>uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems.</li>
</ul>
<p><img alt="Screen Shot 2022-02-07 at 2.17.19 PM.png" src="../img/Screen_Shot_2022-02-07_at_2.17.19_PM.png" /></p>
<h1 id="odometry-estimation">Odometry Estimation</h1>
<p>We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale.</p>
<h2 id="visual-odometry">Visual Odometry</h2>
<p>Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses.</p>
<aside>
ðŸ’¡ Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors.

</aside>

<h3 id="supervised-learning-of-vo">Supervised Learning of VO</h3>
<p>We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems.</p>
<aside>
ðŸ’¡ At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images.

</aside>

<p><strong>DeepVO</strong></p>
<p>DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry.</p>
<blockquote>
<p>The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning.
</p>
</blockquote>
<p><img alt="Screen Shot 2022-02-07 at 2.55.51 PM.png" src="../img/Screen_Shot_2022-02-07_at_2.55.51_PM.png" /></p>
<p>Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. </p>
<aside>
ðŸ’¡ Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation.

</aside>

<p>Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets.</p>
<p>The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. </p>
<p>To recover the optimal parameters <span class="arithmatex">\(\boldsymbol{\theta}^*\)</span> of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations <span class="arithmatex">\(\hat{\textbf{p}} \in \mathbb{R}^3\)</span> and euler angle based rotations <span class="arithmatex">\(\hat{\boldsymbol{\phi}} \in \mathbb{R}^3\)</span>:</p>
<p><img alt="Screen Shot 2022-02-07 at 3.05.39 PM.png" src="../img/Screen_Shot_2022-02-07_at_3.05.39_PM.png" /></p>
<p>DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). </p>
<aside>
ðŸ’¡ Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information.

</aside>

<p>This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric.</p>
<h2 id="unsupervised-learning-of-vo">Unsupervised Learning of VO</h2>
<p>This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal.</p>
<p>As shown in Figure 4(b),</p>
<aside>
ðŸ’¡ a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images.

</aside>

<p>The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image <span class="arithmatex">\(\textbf{I}_s\)</span>, the view synthesis task is to generate a synthetic target image <span class="arithmatex">\(\textbf{I}_t\)</span>. A pixel of source image <span class="arithmatex">\(\textbf{I}_s(p_s)\)</span> is projected onto a target view <span class="arithmatex">\(\textbf{I}_t(p_t)\)</span> via:</p>
<p><img alt="Screen Shot 2022-02-07 at 3.20.06 PM.png" src="../img/Screen_Shot_2022-02-07_at_3.20.06_PM.png" /></p>
<p>where <span class="arithmatex">\(\textbf{K}\)</span> is the cameraâ€™s intrinsic matrix, <span class="arithmatex">\(\textbf{T}_{tâ†’s}\)</span> denotes the camera motion matrix from target frame to source frame, and <span class="arithmatex">\(\textbf{D}_t(p_t)\)</span> denotes the per-pixel depth maps in the target frame. </p>
<aside>
ðŸ’¡ The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one.

</aside>

<p><img alt="Screen Shot 2022-02-07 at 3.38.24 PM.png" src="../img/Screen_Shot_2022-02-07_at_3.38.24_PM.png" /></p>
<p>where <span class="arithmatex">\(p\)</span> denotes pixel coordinates, <span class="arithmatex">\(\textbf{I}_t\)</span> is the target image, and <span class="arithmatex">\(\hat{\textbf{I}}_s\)</span> is the synthetic target image generated from the source image <span class="arithmatex">\(\textbf{I}_s\)</span>.</p>
<p>However, there are basically two main problems that remained unsolved in the original work:</p>
<ol>
<li>this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use.</li>
<li>the photometric loss assumes that the scene is static and without camera occlusions. </li>
</ol>
<h3 id="hybrid-vo">Hybrid VO</h3>
<aside>
ðŸ’¡ Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework.

</aside>

<p>Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model.</p>
<p>A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. </p>
<blockquote>
<p>Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.
</p>
</blockquote>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2019 - 2022 Pengfei Ren
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://github.com/rpfly3" target="_blank" rel="noopener" title="Pengfei on Twitter" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "navigation.tabs.sticky"], "search": "../../../../assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.8492ddcf.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Pengfei's Docs","title":"Home"},{"location":"#welcome-to-pengfeis-docs","text":"","title":"Welcome to Pengfei's Docs"},{"location":"about/","text":"About Me","title":"About"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"av/papers/","text":"Papers Detection Image Classification 2021 ViT : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR 2021) [ notes ] 2020 RegNet : Designing Network Design Spaces (CVPR 2020) [notes] 2019 EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019) [notes] 2D Object Detection 2022 YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors [ notes ][ code ] 2021 Deformable DETR: Deformable transformers for end-to-end object detection (ICLR 2021) [notes] 2020 EfficientDet: Scalable and Efficient Object Detection (CVPR 2020) [notes] DETR : End-to-End Object Detection with Transformers (ECCV 2020) [ notes ] YOLOv4: Optimal Speed and Accuracy of Object Detection [ notes ][ code ] 2019 FCOS: Fully Convolutional One-Stage Object Detection (ICCV 2019) [ notes ] Objects as Points (ICCV 2019) [ notes ] 2012 AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ] 3D Object Detection BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers (ECCV 2022) [ notes ][ code ] Lane Detection 2022 PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark (ECCV 2022 Oral) [ notes ] 2021 LaneATT : Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021) [notes] Focus on Local: Detecting Lane Marker from Bottom Up via Key Point (CVPR 2021) [notes] HDMapNet: An Online HD Map Construction and Evaluation Framework (CVPR 2021) [notes] Structure Guided Lane Detection (IJCAI 2021) [notes] 2018 LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ] Labeling 2022 MTrans : Multimodal Transformer for Automatic 3D Annotation and Object Detection [ notes ] code MPPNet: Multi-Frame Feature Intertwining with Proxy Points for 3D Temporal Object Detection 2021 Offboard 3D Object Detection from Point Cloud Sequences (CVPR 2021) [notes] Auto4D: Learning to Label 4D Objects from Sequential Point Clouds [notes] SLAM 2020 A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ] Tracking Misc 2020 NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020 Oral) [ notes ]","title":"Papers"},{"location":"av/papers/#papers","text":"","title":"Papers"},{"location":"av/papers/#detection","text":"","title":"Detection"},{"location":"av/papers/#image-classification","text":"","title":"Image Classification"},{"location":"av/papers/#2021","text":"ViT : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR 2021) [ notes ]","title":"2021"},{"location":"av/papers/#2020","text":"RegNet : Designing Network Design Spaces (CVPR 2020) [notes]","title":"2020"},{"location":"av/papers/#2019","text":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019) [notes]","title":"2019"},{"location":"av/papers/#2d-object-detection","text":"","title":"2D Object Detection"},{"location":"av/papers/#2022","text":"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors [ notes ][ code ]","title":"2022"},{"location":"av/papers/#2021_1","text":"Deformable DETR: Deformable transformers for end-to-end object detection (ICLR 2021) [notes]","title":"2021"},{"location":"av/papers/#2020_1","text":"EfficientDet: Scalable and Efficient Object Detection (CVPR 2020) [notes] DETR : End-to-End Object Detection with Transformers (ECCV 2020) [ notes ] YOLOv4: Optimal Speed and Accuracy of Object Detection [ notes ][ code ]","title":"2020"},{"location":"av/papers/#2019_1","text":"FCOS: Fully Convolutional One-Stage Object Detection (ICCV 2019) [ notes ] Objects as Points (ICCV 2019) [ notes ]","title":"2019"},{"location":"av/papers/#2012","text":"AlexNet : ImageNet Classification with Deep Convolutional Neural Networks (NIPS 2012) [ notes ]","title":"2012"},{"location":"av/papers/#3d-object-detection","text":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers (ECCV 2022) [ notes ][ code ]","title":"3D Object Detection"},{"location":"av/papers/#lane-detection","text":"","title":"Lane Detection"},{"location":"av/papers/#2022_1","text":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark (ECCV 2022 Oral) [ notes ]","title":"2022"},{"location":"av/papers/#2021_2","text":"LaneATT : Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021) [notes] Focus on Local: Detecting Lane Marker from Bottom Up via Key Point (CVPR 2021) [notes] HDMapNet: An Online HD Map Construction and Evaluation Framework (CVPR 2021) [notes] Structure Guided Lane Detection (IJCAI 2021) [notes]","title":"2021"},{"location":"av/papers/#2018","text":"LaneNet : Towards End-to-End Lane Detection: an Instance Segmentation Approach [ notes ]","title":"2018"},{"location":"av/papers/#labeling","text":"","title":"Labeling"},{"location":"av/papers/#2022_2","text":"MTrans : Multimodal Transformer for Automatic 3D Annotation and Object Detection [ notes ] code MPPNet: Multi-Frame Feature Intertwining with Proxy Points for 3D Temporal Object Detection","title":"2022"},{"location":"av/papers/#2021_3","text":"Offboard 3D Object Detection from Point Cloud Sequences (CVPR 2021) [notes] Auto4D: Learning to Label 4D Objects from Sequential Point Clouds [notes]","title":"2021"},{"location":"av/papers/#slam","text":"","title":"SLAM"},{"location":"av/papers/#2020_2","text":"A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence [ notes ]","title":"2020"},{"location":"av/papers/#tracking","text":"","title":"Tracking"},{"location":"av/papers/#misc","text":"","title":"Misc"},{"location":"av/papers/#2020_3","text":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (ECCV 2020 Oral) [ notes ]","title":"2020"},{"location":"av/detection/","text":"Detection Modules Attention101 Transformer Vision Transformer Monocular BEV Perception Architecture YOLO","title":"Detection"},{"location":"av/detection/#detection","text":"","title":"Detection"},{"location":"av/detection/#modules","text":"Attention101 Transformer Vision Transformer Monocular BEV Perception","title":"Modules"},{"location":"av/detection/#architecture","text":"YOLO","title":"Architecture"},{"location":"av/detection/alex_net/alex_net/","text":"AlexNet Rich feature hierarchies for accurate object detection and semantic segmentation Abstract In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Introduction AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption. Object detection with R-CNN Module design Region proposals R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work. Feature extraction They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly \\( \\(p\\) \\) pixels of warped image context around the original box (they use \\( \\(p = 16\\) \\) ). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible. Test-time detection At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#alexnet","text":"","title":"AlexNet"},{"location":"av/detection/alex_net/alex_net/#rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation","text":"","title":"Rich feature hierarchies for accurate object detection and semantic segmentation"},{"location":"av/detection/alex_net/alex_net/#abstract","text":"In this paper, they propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to previous best results on VOC2012 - achieving a mAP of 53.3%. The proposed approach combines two key insights: one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects. when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.","title":"Abstract"},{"location":"av/detection/alex_net/alex_net/#introduction","text":"AlexNet is the revival of CNN. To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge? They answer this question by bridging the gap between image classification and object detection. To achieve this result they focused on two problems: localizing objects with a deep network training a high-capacity with only a small quantity of annotated detection data Unlike image classification, detection requires localizing objects within an image. The potential solutions include framing localization as regression problem. This has been shown not working well. building a sliding-widow detector. However, units high up in CNN have very large receptive fields and strides in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge. They solve the CNN localization problem by operating within the recognition using regions paradigm, which has been successful for both object detection and semantic segmentation. generates around 2000 category-independent region proposals for the input image extracts a fixed-length feature vector from each proposal using a CNN Classifies each region with category-specific linear SVMs. They use a simple technique ( afine image warping ) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. The second challenge faced in detection is that labeled data is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training , followed by supervised fine-tuning This paper shows that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domain specific fine-tuning on a small dataset (PASCAL), is an effective paradigm for training high-capacity CNNs when data is scarce. ( Transferring Learning ) In DeCAF, the author shows that AlexNet can be used as a blcakbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaption.","title":"Introduction"},{"location":"av/detection/alex_net/alex_net/#object-detection-with-r-cnn","text":"","title":"Object detection with R-CNN"},{"location":"av/detection/alex_net/alex_net/#module-design","text":"","title":"Module design"},{"location":"av/detection/alex_net/alex_net/#region-proposals","text":"R-CNN is agnostic to the particular region proposal method, however, they use selective search to enable a controlled comparison with prior detection work.","title":"Region proposals"},{"location":"av/detection/alex_net/alex_net/#feature-extraction","text":"They extract a 4096-dimensional feature vector from each region proposal using the Caffe implementation of AlexNet. Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. Regardless of the size or aspect ratio of the candidate region, they warp all pixels in a tight bounding box around it to the required size. Prior to warping, they dilate the tight bounding box so that at the warped size there are exactly \\( \\(p\\) \\) pixels of warped image context around the original box (they use \\( \\(p = 16\\) \\) ). Object proposal transformations : tightest square with context: enclosing each object proposal inside the tighest square and then scales (isotropically) the image contained in that square to the CNN input size. tightest square without context: excluding the image content that surrounds the original object proposal. warp: anisotropically scales each object proposal to the CNN input size. Obviously more alternatives are possible.","title":"Feature extraction"},{"location":"av/detection/alex_net/alex_net/#test-time-detection","text":"At test time, run selective search on the test image to extract around 2000 region proposals warp each proposals and forward propagate it through the CNN in order to compute features for each class, they score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, they apply a greedy NMS (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.","title":"Test-time detection"},{"location":"av/detection/architecture/yolo/","text":"YOLO v1 [ notes ] v2 [ notes ] v3 [ notes ]","title":"YOLO"},{"location":"av/detection/architecture/yolo/#yolo","text":"","title":"YOLO"},{"location":"av/detection/architecture/yolo/#v1","text":"[ notes ]","title":"v1"},{"location":"av/detection/architecture/yolo/#v2","text":"[ notes ]","title":"v2"},{"location":"av/detection/architecture/yolo/#v3","text":"[ notes ]","title":"v3"},{"location":"av/detection/bevformer/bevformer/","text":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers Abstract BEVFormer learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, they design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, they propose temporal self-attention to recurrently fuse the history BEV information. Introduction Visual perception of the surrounding scene in autonomous driving is expected to predict the 3D bounding boxes or the semantic maps from 2D cues given by multiple cameras. Although previous map segmentation methods demonstrate BEV's effectiveness, BEV-based approaches have not shown significant advantage over other paradigm in 3D object detections. Note The underlying reason is that the 3D object detection task requires strong BEV features to support accurate 3D bounding box prediction, but generating BEV from the 2D planes is ill-posed. Motivations A popular BEV framework that generates BEV features is based on depth information, but this paradigm is sensitive to the accuracy of depth values or the depth distributions. The detection performance of BEV-based methods is thus subject to compounding errors, and inaccurate BEV features can seriously hurt the final performance. The significant challenges are that autonomous driving is time-critical and objects in the scene change rapidly, and thus simply stacking BEV features of cross timestamps bring extra computational cost and interference information, which might not be ideal. BEVFormer Overview BEVFormer contains three key designs: grid-shaped BEV queries to fuse spatial and temporal features via attention mechanisms flexibly. spatial cross-attention module to aggregate the spatial features from multi-camera images. temporal self-attention module to extract temporal information from histogram BEV features, which benefits the velocity estimation of moving objects and the detection of heavily occluded objects, while brining negligible computational overhead. With the unified features generated by BEVFormer, the model can collaborate with different task-specific heads such as Deformable DETR and mask decoder, for end-to-end 3D object detection and map segmentation. Key Contributions BEVFromer, a spatiotemporal transformer encoder that projects multi-camera and/or timestamp input to BEV representations. BEVFormer Overall Architecture BEVFormer has 6 encoder layers, each of which follows the conventional structure of transformers, except for three tailored designs, namely BEV queries, spatial cross-attention, and temporal self-attention.","title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"},{"location":"av/detection/bevformer/bevformer/#bevformer-learning-birds-eye-view-representation-from-multi-camera-images-via-spatiotemporal-transformers","text":"Abstract BEVFormer learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, they design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, they propose temporal self-attention to recurrently fuse the history BEV information.","title":"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"},{"location":"av/detection/bevformer/bevformer/#introduction","text":"Visual perception of the surrounding scene in autonomous driving is expected to predict the 3D bounding boxes or the semantic maps from 2D cues given by multiple cameras. Although previous map segmentation methods demonstrate BEV's effectiveness, BEV-based approaches have not shown significant advantage over other paradigm in 3D object detections. Note The underlying reason is that the 3D object detection task requires strong BEV features to support accurate 3D bounding box prediction, but generating BEV from the 2D planes is ill-posed. Motivations A popular BEV framework that generates BEV features is based on depth information, but this paradigm is sensitive to the accuracy of depth values or the depth distributions. The detection performance of BEV-based methods is thus subject to compounding errors, and inaccurate BEV features can seriously hurt the final performance. The significant challenges are that autonomous driving is time-critical and objects in the scene change rapidly, and thus simply stacking BEV features of cross timestamps bring extra computational cost and interference information, which might not be ideal. BEVFormer Overview BEVFormer contains three key designs: grid-shaped BEV queries to fuse spatial and temporal features via attention mechanisms flexibly. spatial cross-attention module to aggregate the spatial features from multi-camera images. temporal self-attention module to extract temporal information from histogram BEV features, which benefits the velocity estimation of moving objects and the detection of heavily occluded objects, while brining negligible computational overhead. With the unified features generated by BEVFormer, the model can collaborate with different task-specific heads such as Deformable DETR and mask decoder, for end-to-end 3D object detection and map segmentation. Key Contributions BEVFromer, a spatiotemporal transformer encoder that projects multi-camera and/or timestamp input to BEV representations.","title":"Introduction"},{"location":"av/detection/bevformer/bevformer/#bevformer","text":"","title":"BEVFormer"},{"location":"av/detection/bevformer/bevformer/#overall-architecture","text":"BEVFormer has 6 encoder layers, each of which follows the conventional structure of transformers, except for three tailored designs, namely BEV queries, spatial cross-attention, and temporal self-attention.","title":"Overall Architecture"},{"location":"av/detection/center_net/center_net/","text":"Objects as Points Abstract They model an object as a single point -- the center point of its bounding box. The detector uses key-point estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Introduction Current object detectors represent each object through an axis-aligned bounding box that tightly encompasses the object. They then reduce object detection to image classification of an extensive number of potential object bounding boxes. For each bounding box, the classifier determines if the image content is a specific object or background. One-stage detectors slide a complex arrangement of possible bounding boxes, called anchors, over the image and classify them directly without specifying the box content. Two-stage detectors recompute image features for each potential box, then classify those features. Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. Warning This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Sliding window based object detectors are however a bit wasteful, as they need to enumerate all possible object locations and dimensions. In this paper, they provide a much simpler and more efficient alternative. They represent objects by a single point at their bounding box center.Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard key-point estimation problem. They simply feed the input image to a fully convolutional network that generates a heatmap. Peaks in this heatmap correspond to object centers. Image features at each peak predict the objects bounding box height and weight. Hint Inference is a single network forward-pass, without non-maximal suppression for post-processing. Related work Anchor-based Detector Their approach is closely related to anchor-based one-stage approaches. Hint A center point can be seen as a single shape-agnostic anchor. However, there are a few important differences. CenterNet assigns the \"anchor\" based solely on location, not box overlap. They have no manual thresholds for foreground and background classification. They only have one positive \"anchor\" per object, and hence do not need Non-Maxima Suppression (NMS). CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors (output stride of 16). This eliminates the need for multiple anchors. Object detection by key-point estimation CornerNet detects two bounding box corners as key points, while ExtremeNet detects the top-, left-, bottom-, right-most, and center points of all objects. ==Both these methods build on the same robust key point estimation network as CenterNet. Warning However, they require a combinatorial grouping stage after key point detection, which significantly slows down each algorithm. Preliminary They use several different fully-convolutional encoder-decoder networks to predict key points from an image: a stacked hourglass network, up-convolutional residual networks (ResNet), and deep layer aggregation (DLA). They train the key point prediction network following CornerNet. For each ground truth key point, they compute a low-resolution equivalent. They then splat all ground truth key points onto a heatmap using a Gaussian kernel, where \\(\\sigma_p\\) is an object size-adaptive standard deviation. If two Gaussian distributions of the same class overlap, they take the element-wise maximum. The training objective is a penalty-reduced pixel wise logistic regression with focal loss. \\(N\\) is the number of key points in the image, and the normalization by \\(N\\) is chosen as to normalize all positive focal loss instances to 1. Note To recover the discretization error caused by the output stride, they additionally predict a local offset for each center point. All classes \\(c\\) share the same offset prediction. Objects as Points To limit the computational burden, they use a single size predictor for all object categories. From points to bounding boxes At inference time, they first extract the peaks in the heatmap for each category independently. They detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. They use the key point values as a measure of its detection confidence. Note The peak key point extraction serves as a sufficient NMS alternative and can be implemented efficiently on device using a 3 x 3 max pooling operation.","title":"Objects as Points"},{"location":"av/detection/center_net/center_net/#objects-as-points","text":"Abstract They model an object as a single point -- the center point of its bounding box. The detector uses key-point estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose.","title":"Objects as Points"},{"location":"av/detection/center_net/center_net/#introduction","text":"Current object detectors represent each object through an axis-aligned bounding box that tightly encompasses the object. They then reduce object detection to image classification of an extensive number of potential object bounding boxes. For each bounding box, the classifier determines if the image content is a specific object or background. One-stage detectors slide a complex arrangement of possible bounding boxes, called anchors, over the image and classify them directly without specifying the box content. Two-stage detectors recompute image features for each potential box, then classify those features. Post-processing, namely non-maxima suppression, then removes duplicated detections for the same instance by computing bounding box IoU. Warning This post-processing is hard to differentiate and train, hence most current detectors are not end-to-end trainable. Sliding window based object detectors are however a bit wasteful, as they need to enumerate all possible object locations and dimensions. In this paper, they provide a much simpler and more efficient alternative. They represent objects by a single point at their bounding box center.Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard key-point estimation problem. They simply feed the input image to a fully convolutional network that generates a heatmap. Peaks in this heatmap correspond to object centers. Image features at each peak predict the objects bounding box height and weight. Hint Inference is a single network forward-pass, without non-maximal suppression for post-processing.","title":"Introduction"},{"location":"av/detection/center_net/center_net/#related-work","text":"","title":"Related work"},{"location":"av/detection/center_net/center_net/#anchor-based-detector","text":"Their approach is closely related to anchor-based one-stage approaches. Hint A center point can be seen as a single shape-agnostic anchor. However, there are a few important differences. CenterNet assigns the \"anchor\" based solely on location, not box overlap. They have no manual thresholds for foreground and background classification. They only have one positive \"anchor\" per object, and hence do not need Non-Maxima Suppression (NMS). CenterNet uses a larger output resolution (output stride of 4) compared to traditional object detectors (output stride of 16). This eliminates the need for multiple anchors.","title":"Anchor-based Detector"},{"location":"av/detection/center_net/center_net/#object-detection-by-key-point-estimation","text":"CornerNet detects two bounding box corners as key points, while ExtremeNet detects the top-, left-, bottom-, right-most, and center points of all objects. ==Both these methods build on the same robust key point estimation network as CenterNet. Warning However, they require a combinatorial grouping stage after key point detection, which significantly slows down each algorithm.","title":"Object detection by key-point estimation"},{"location":"av/detection/center_net/center_net/#preliminary","text":"They use several different fully-convolutional encoder-decoder networks to predict key points from an image: a stacked hourglass network, up-convolutional residual networks (ResNet), and deep layer aggregation (DLA). They train the key point prediction network following CornerNet. For each ground truth key point, they compute a low-resolution equivalent. They then splat all ground truth key points onto a heatmap using a Gaussian kernel, where \\(\\sigma_p\\) is an object size-adaptive standard deviation. If two Gaussian distributions of the same class overlap, they take the element-wise maximum. The training objective is a penalty-reduced pixel wise logistic regression with focal loss. \\(N\\) is the number of key points in the image, and the normalization by \\(N\\) is chosen as to normalize all positive focal loss instances to 1. Note To recover the discretization error caused by the output stride, they additionally predict a local offset for each center point. All classes \\(c\\) share the same offset prediction.","title":"Preliminary"},{"location":"av/detection/center_net/center_net/#objects-as-points_1","text":"To limit the computational burden, they use a single size predictor for all object categories.","title":"Objects as Points"},{"location":"av/detection/center_net/center_net/#from-points-to-bounding-boxes","text":"At inference time, they first extract the peaks in the heatmap for each category independently. They detect all responses whose value is greater or equal to its 8-connected neighbors and keep the top 100 peaks. They use the key point values as a measure of its detection confidence. Note The peak key point extraction serves as a sufficient NMS alternative and can be implemented efficiently on device using a 3 x 3 max pooling operation.","title":"From points to bounding boxes"},{"location":"av/detection/detr/detr/","text":"End-to-End Object Detection with Transformers Abstract They present a new method that views object detection as a direct set prediction problem . This approach removes the need for many hand-designed components like a non-maximum suppression or anchor generation that explicitly encode prior knowledge about the task. The main ingredients of the new framework DETR ( DE tection TR ansformer) are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. DETR can be easily generalized to produce panoptic segmentation. Introduction The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals, anchors, or window centers. Note Their performance are significantly influenced by post-processing steps to collapse near-duplicate predictions, by the design of the anchor sets and by heuristics that assign target boxes to anchors. DETR This paper streamline the training pipeline by viewing object detection as a direct set prediction problem. They adopt an encoder-decoder architecture based on transformers. Note The self-attention mechanisms of transformers, which explicitly model all pair wise interactions between elements in a sequence, make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions. The DETR predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with (non-autoregressive) parallel decoding. Training settings for DETR differ from standard object detectors in multiple ways. The new model requires extra-long training schedule and benefits from auxiliary decoding losses in the transformer. Related Work Set Prediction In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match. Transformers and Parallel Decoding Note One of the main advantages of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers were first used in auto-regressive models, following early sequence-to-sequence models, generating output tokens one by one. However, the prohibitive inference cost (proportional to output length, and hard to batch) lead to the development of parallel sequence generation. Object Detection Most modern object detection methods make predictions make predictions relative to some initial guesses. Recent work demonstrate that the final performance of these systems heavily depends on the exact way these initial guesses are set. The DETR Model Two ingredients are essential for direct set predictions in detection: a set prediction loss that forces unique matching between predicted and ground truth boxes. an architecture that predicts (in a single pass) a set of objects and models their relation. Object Detection Set Prediction Loss DETR infers a fixed-size set of \\(N\\) predictions, in a single pass through the decoder, where \\(N\\) is set to be significantly larger than the typical number of objects in an image. Note One of the main difficulties of training is to score predicted objects (class, position, size) with respect to the ground truth. The proposed loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses. The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. In the bipartite graph matching, they use probabilities for matching classes. But they use log-probabilities in the Hungarian loss. The most commonly-used l1 loss will have different scales for small and large boxes even if relative errors are similar. To mitigate this issue they use a linear combination of the l1 loss and the generalized IoU loss. DETR Architecture Transformer Encoder The encoder expects a sequence as input, hence they collapse the spatial dimensions of \\(z_0\\) into one dimension, resulting a \\(d \\times HW\\) feature map. Transformer Decoder The difference with the original transformer is that the new model decodes the \\(N\\) objects in parallel at each decoder layer, while the original transformer uses an auto-regressive model that predicts the output sequence one element at a time. Since the decoder is also permutation-invariant, the \\(N\\) input embeddings must be different to produce different results. These input embeddings are learnt positional encodings that they refer to as object queries, and similarly to the encoder, they add them to the input of each attention layer. Note The \\(N\\) object queries are transformed into an output embedding by the decoder in parallel. They are then independently decoded into box coordinates and class labels by a feed forward network, resulting \\(N\\) final predictions. Prediction Feed-forward Networks (FFNs) The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension \\(d\\) , and a linear projection layer. Auxiliary Decoding Losses They found it is helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number of objects of each class. They add prediction FNNs and Hungarian loss after each decoder layer. All prediction FFNs share their parameters. They use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers.","title":"End-to-End Object Detection with Transformers"},{"location":"av/detection/detr/detr/#end-to-end-object-detection-with-transformers","text":"Abstract They present a new method that views object detection as a direct set prediction problem . This approach removes the need for many hand-designed components like a non-maximum suppression or anchor generation that explicitly encode prior knowledge about the task. The main ingredients of the new framework DETR ( DE tection TR ansformer) are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. DETR can be easily generalized to produce panoptic segmentation.","title":"End-to-End Object Detection with Transformers"},{"location":"av/detection/detr/detr/#introduction","text":"The goal of object detection is to predict a set of bounding boxes and category labels for each object of interest. Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals, anchors, or window centers. Note Their performance are significantly influenced by post-processing steps to collapse near-duplicate predictions, by the design of the anchor sets and by heuristics that assign target boxes to anchors.","title":"Introduction"},{"location":"av/detection/detr/detr/#detr","text":"This paper streamline the training pipeline by viewing object detection as a direct set prediction problem. They adopt an encoder-decoder architecture based on transformers. Note The self-attention mechanisms of transformers, which explicitly model all pair wise interactions between elements in a sequence, make these architectures particularly suitable for specific constraints of set prediction such as removing duplicate predictions. The DETR predicts all objects at once, and is trained end-to-end with a set loss function which performs bipartite matching between predicted and ground-truth objects. Compared to most previous work on direct set prediction, the main features of DETR are the conjunction of the bipartite matching loss and transformers with (non-autoregressive) parallel decoding. Training settings for DETR differ from standard object detectors in multiple ways. The new model requires extra-long training schedule and benefits from auxiliary decoding losses in the transformer.","title":"DETR"},{"location":"av/detection/detr/detr/#related-work","text":"","title":"Related Work"},{"location":"av/detection/detr/detr/#set-prediction","text":"In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian algorithm, to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match.","title":"Set Prediction"},{"location":"av/detection/detr/detr/#transformers-and-parallel-decoding","text":"Note One of the main advantages of attention-based models is their global computations and perfect memory, which makes them more suitable than RNNs on long sequences. Transformers were first used in auto-regressive models, following early sequence-to-sequence models, generating output tokens one by one. However, the prohibitive inference cost (proportional to output length, and hard to batch) lead to the development of parallel sequence generation.","title":"Transformers and Parallel Decoding"},{"location":"av/detection/detr/detr/#object-detection","text":"Most modern object detection methods make predictions make predictions relative to some initial guesses. Recent work demonstrate that the final performance of these systems heavily depends on the exact way these initial guesses are set.","title":"Object Detection"},{"location":"av/detection/detr/detr/#the-detr-model","text":"Two ingredients are essential for direct set predictions in detection: a set prediction loss that forces unique matching between predicted and ground truth boxes. an architecture that predicts (in a single pass) a set of objects and models their relation.","title":"The DETR Model"},{"location":"av/detection/detr/detr/#object-detection-set-prediction-loss","text":"DETR infers a fixed-size set of \\(N\\) predictions, in a single pass through the decoder, where \\(N\\) is set to be significantly larger than the typical number of objects in an image. Note One of the main difficulties of training is to score predicted objects (class, position, size) with respect to the ground truth. The proposed loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses. The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes. In the bipartite graph matching, they use probabilities for matching classes. But they use log-probabilities in the Hungarian loss. The most commonly-used l1 loss will have different scales for small and large boxes even if relative errors are similar. To mitigate this issue they use a linear combination of the l1 loss and the generalized IoU loss.","title":"Object Detection Set Prediction Loss"},{"location":"av/detection/detr/detr/#detr-architecture","text":"","title":"DETR Architecture"},{"location":"av/detection/detr/detr/#transformer-encoder","text":"The encoder expects a sequence as input, hence they collapse the spatial dimensions of \\(z_0\\) into one dimension, resulting a \\(d \\times HW\\) feature map.","title":"Transformer Encoder"},{"location":"av/detection/detr/detr/#transformer-decoder","text":"The difference with the original transformer is that the new model decodes the \\(N\\) objects in parallel at each decoder layer, while the original transformer uses an auto-regressive model that predicts the output sequence one element at a time. Since the decoder is also permutation-invariant, the \\(N\\) input embeddings must be different to produce different results. These input embeddings are learnt positional encodings that they refer to as object queries, and similarly to the encoder, they add them to the input of each attention layer. Note The \\(N\\) object queries are transformed into an output embedding by the decoder in parallel. They are then independently decoded into box coordinates and class labels by a feed forward network, resulting \\(N\\) final predictions.","title":"Transformer Decoder"},{"location":"av/detection/detr/detr/#prediction-feed-forward-networks-ffns","text":"The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension \\(d\\) , and a linear projection layer.","title":"Prediction Feed-forward Networks (FFNs)"},{"location":"av/detection/detr/detr/#auxiliary-decoding-losses","text":"They found it is helpful to use auxiliary losses in decoder during training, especially to help the model output the correct number of objects of each class. They add prediction FNNs and Hungarian loss after each decoder layer. All prediction FFNs share their parameters. They use an additional shared layer-norm to normalize the input to the prediction FFNs from different decoder layers.","title":"Auxiliary Decoding Losses"},{"location":"av/detection/fcos/fcos/","text":"FCOS: Fully Convolutional One-Stage Object Detection Abstract They propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. The proposed detector FCOS is anchor box free, as well as proposal free. avoid the complicated computation related to anchor boxes such as calculating overlapping during training. avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. FCOS: a much simpler and flexible detection framework achieving improved detection accuracy. Introduction Anchor Based Detection All current mainstream detectors such as Faster R-CNN, SSD and YOLOv2, v3 rely on a set of pre-defined anchor boxes and it has long been believed that the use of anchor boxes is the key to detectors' success. Despite their great success, it is important to note that anchor-based detectors suffer some drawbacks: Detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes. These hyper-parameters need to be carefully tuned in anchor-based detectors. Even with careful design, because the scales and aspect ratios of anchor boxes are kept fixed, detectors encounter difficulties to deal with object candidates with large shape variations, particularly for small objects. The pre-defined anchor boxes also hamper the generalization ability of detectors, as they need to be re-designed on new detection tasks with different object sizes or aspect ratios. In order to achieve a high recall rate, an anchor-based detector is required to densely place anchor boxes on the input image. Most of these anchor boxes are labelled as negative samples during training. The excessive number of negative samples aggravates the imbalance between positive and negative samples in training. Anchor boxes also involve complicated computation such as calculating the IoU scores with ground-truth bounding boxes. Fully Convolutional Networks (FCNs) In the literature, some works attempted to leverage the FCNs-based framework for object detection such as DenseBox. However, to handle the bounding boxes with different sizes, DenseBox crops and resizes training images to a fixed scale. Thus DenseBox has to perform detection on image pyramids, which is against FCN's philosophy of computing all convolutions once. In addition, it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes. Warning The highly overlapped bounding boxes result in an intractable ambiguity: it is not clear w.r.t. which bounding box to regress for the pixels in the overlapped regions. They took a closer look at the issue and show that with FPN this ambiguity can be largely eliminated. their method can obtain comparable detection accuracy with those traditional anchor based detectors. their method may produce a number of low-quality predicted bounding boxes at the locations that are far from the center of an target object. In order to suppress these low-quality detections, they introduce a novel \"center-ness\" branch (only one layer) to predict the deviation of a pixel to the center of its corresponding bounding boxes. Related Work Anchor-based Detectors. Hint Anchor-based detectors inherit the ideas from traditional sliding-window and proposal based detectors such as Fast R-CNN. In anchor-based detectors, the anchor boxes can be viewed as pre-defined sliding windows or proposals, which are classified as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. Therefore, the anchor boxes in these detectors may be viewed as training samples. The design of anchor boxes are popularized by Faster R-CNN in its RPNs, SSD and YOLOv2, and has become the convention in a modern detector. Anchor-free Detectors YOLOv1 The most popular anchor-free detector might be YOLOv2. Instead of using anchor boxes, YOLOv1 predicts bounding boxes at points near the center of objects. Note Only the points near the center are used since they are considered to be able to produce higher quality detection. However, since only points near the center are used to predict bounding boxes, YOLOv1 suffers from low recall. Compared to YOLOv1, FCOS takes advantages of all points in a ground truth bounding box to predict the bounding boxes and the low-quality detected bounding boxes are suppressed by the proposed \"center-ness\" branch. CornerNet CornerNet is a recently proposed one-stage anchor-free detector, which detects a pair of corners of a bounding box and groups them to form the final detected bounding box. CornerNet requires much more complicated post processing to group the pairs of corners belonging to the same instance. An extra distance metric is learned for the purpose of grouping. Proposed Approach Fully Convolutional One-Stage Object Detector Different from anchor-based detectors, which consider the location on the input image as the center of (multiple) anchor boxes and regress the target bounding box with these anchor boxes as references, they directly regress the target bounding box at the location. In other words, their detector directly views locations as training samples instead of anchor boxes in anchor-based detectors, which is the same as FCNs for semantic segmentation. Note Specifically, location \\((x, y)\\) is considered as a positive sample if it falls into any ground-truth box and the class labels \\(c^*\\) of the location is the class label of the ground truth box. Otherwise it is a negative sample and \\(c^* = 0\\) (background class). Besides the label for classification, they also have a 4D real vector being the regression targets for the location. If a location falls into multiple bounding boxes, it is considered as an ambiguous sample. They simply choose the bounding box with minimal area as its regression target. Hint It is worth noting that FCOS can leverage as many foreground samples as possible to train the regressor. It is different from anchor-based detectors, which only consider the anchor boxes with a highly enough IoU with ground-truth boxes as positive samples. Network Outputs It is worth noting that FCOS has \\(9 \\times\\) fewer network output variables than the popular anchor based detectors with 9 anchor boxes per location. Multi-level Prediction with FPN for FCOS Two possible issues of the proposed FCOS can be resolved with multi-level prediction with FPN. The large stride (e.g., \\(16 \\times\\) ) of the final feature maps in a CNN can result in a relatively low best possible recall (BPR). Overlaps in ground-truth boxes can cause intractable ambiguity. Unlike anchor-based detectors, which assign anchor boxes with different sizes to different feature levels, they directly limit the range of bounding box regression for each level. Since objects with different sizes are assigned to different feature levels and most overlapping happens between objects with considerably different sizes. If a location, even with multi-level prediction used, is still assigned to more than one ground-truth boxes, they simply choose the ground-truth box with minimal area as its target. Hint They share the heads between different feature levels, not only making the detector parameter-efficient but also improving the detection performance. Instead of using the standard \\(exp(x)\\) , they make use of \\(exp(s_ix)\\) with a trainable scalar \\(s_i\\) to automatically adjust the base of the exponential function for feature level \\(P_i\\) , which slightly improves the detection performance. Center-ness for FCOS After using multi-level prediction in FCOS, there is still a performance gap between FCOS and anchor-based detectors. They observed that it is due to a lot of low-quality predicted bounding boxes produced by locations far away from the center of an object. They add a single-layer branch, in parallel with the classification branch to predict the \"center-ness\" of a location. The center-ness depicts the normalized distance from the location to the center of the object that the location is responsible for.","title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"location":"av/detection/fcos/fcos/#fcos-fully-convolutional-one-stage-object-detection","text":"Abstract They propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. The proposed detector FCOS is anchor box free, as well as proposal free. avoid the complicated computation related to anchor boxes such as calculating overlapping during training. avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. FCOS: a much simpler and flexible detection framework achieving improved detection accuracy.","title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"location":"av/detection/fcos/fcos/#introduction","text":"Anchor Based Detection All current mainstream detectors such as Faster R-CNN, SSD and YOLOv2, v3 rely on a set of pre-defined anchor boxes and it has long been believed that the use of anchor boxes is the key to detectors' success. Despite their great success, it is important to note that anchor-based detectors suffer some drawbacks: Detection performance is sensitive to the sizes, aspect ratios and number of anchor boxes. These hyper-parameters need to be carefully tuned in anchor-based detectors. Even with careful design, because the scales and aspect ratios of anchor boxes are kept fixed, detectors encounter difficulties to deal with object candidates with large shape variations, particularly for small objects. The pre-defined anchor boxes also hamper the generalization ability of detectors, as they need to be re-designed on new detection tasks with different object sizes or aspect ratios. In order to achieve a high recall rate, an anchor-based detector is required to densely place anchor boxes on the input image. Most of these anchor boxes are labelled as negative samples during training. The excessive number of negative samples aggravates the imbalance between positive and negative samples in training. Anchor boxes also involve complicated computation such as calculating the IoU scores with ground-truth bounding boxes. Fully Convolutional Networks (FCNs) In the literature, some works attempted to leverage the FCNs-based framework for object detection such as DenseBox. However, to handle the bounding boxes with different sizes, DenseBox crops and resizes training images to a fixed scale. Thus DenseBox has to perform detection on image pyramids, which is against FCN's philosophy of computing all convolutions once. In addition, it is believed that these methods do not work well when applied to generic object detection with highly overlapped bounding boxes. Warning The highly overlapped bounding boxes result in an intractable ambiguity: it is not clear w.r.t. which bounding box to regress for the pixels in the overlapped regions. They took a closer look at the issue and show that with FPN this ambiguity can be largely eliminated. their method can obtain comparable detection accuracy with those traditional anchor based detectors. their method may produce a number of low-quality predicted bounding boxes at the locations that are far from the center of an target object. In order to suppress these low-quality detections, they introduce a novel \"center-ness\" branch (only one layer) to predict the deviation of a pixel to the center of its corresponding bounding boxes.","title":"Introduction"},{"location":"av/detection/fcos/fcos/#related-work","text":"","title":"Related Work"},{"location":"av/detection/fcos/fcos/#anchor-based-detectors","text":"Hint Anchor-based detectors inherit the ideas from traditional sliding-window and proposal based detectors such as Fast R-CNN. In anchor-based detectors, the anchor boxes can be viewed as pre-defined sliding windows or proposals, which are classified as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. Therefore, the anchor boxes in these detectors may be viewed as training samples. The design of anchor boxes are popularized by Faster R-CNN in its RPNs, SSD and YOLOv2, and has become the convention in a modern detector.","title":"Anchor-based Detectors."},{"location":"av/detection/fcos/fcos/#anchor-free-detectors","text":"YOLOv1 The most popular anchor-free detector might be YOLOv2. Instead of using anchor boxes, YOLOv1 predicts bounding boxes at points near the center of objects. Note Only the points near the center are used since they are considered to be able to produce higher quality detection. However, since only points near the center are used to predict bounding boxes, YOLOv1 suffers from low recall. Compared to YOLOv1, FCOS takes advantages of all points in a ground truth bounding box to predict the bounding boxes and the low-quality detected bounding boxes are suppressed by the proposed \"center-ness\" branch. CornerNet CornerNet is a recently proposed one-stage anchor-free detector, which detects a pair of corners of a bounding box and groups them to form the final detected bounding box. CornerNet requires much more complicated post processing to group the pairs of corners belonging to the same instance. An extra distance metric is learned for the purpose of grouping.","title":"Anchor-free Detectors"},{"location":"av/detection/fcos/fcos/#proposed-approach","text":"","title":"Proposed Approach"},{"location":"av/detection/fcos/fcos/#fully-convolutional-one-stage-object-detector","text":"Different from anchor-based detectors, which consider the location on the input image as the center of (multiple) anchor boxes and regress the target bounding box with these anchor boxes as references, they directly regress the target bounding box at the location. In other words, their detector directly views locations as training samples instead of anchor boxes in anchor-based detectors, which is the same as FCNs for semantic segmentation. Note Specifically, location \\((x, y)\\) is considered as a positive sample if it falls into any ground-truth box and the class labels \\(c^*\\) of the location is the class label of the ground truth box. Otherwise it is a negative sample and \\(c^* = 0\\) (background class). Besides the label for classification, they also have a 4D real vector being the regression targets for the location. If a location falls into multiple bounding boxes, it is considered as an ambiguous sample. They simply choose the bounding box with minimal area as its regression target. Hint It is worth noting that FCOS can leverage as many foreground samples as possible to train the regressor. It is different from anchor-based detectors, which only consider the anchor boxes with a highly enough IoU with ground-truth boxes as positive samples.","title":"Fully Convolutional One-Stage Object Detector"},{"location":"av/detection/fcos/fcos/#network-outputs","text":"It is worth noting that FCOS has \\(9 \\times\\) fewer network output variables than the popular anchor based detectors with 9 anchor boxes per location.","title":"Network Outputs"},{"location":"av/detection/fcos/fcos/#multi-level-prediction-with-fpn-for-fcos","text":"Two possible issues of the proposed FCOS can be resolved with multi-level prediction with FPN. The large stride (e.g., \\(16 \\times\\) ) of the final feature maps in a CNN can result in a relatively low best possible recall (BPR). Overlaps in ground-truth boxes can cause intractable ambiguity. Unlike anchor-based detectors, which assign anchor boxes with different sizes to different feature levels, they directly limit the range of bounding box regression for each level. Since objects with different sizes are assigned to different feature levels and most overlapping happens between objects with considerably different sizes. If a location, even with multi-level prediction used, is still assigned to more than one ground-truth boxes, they simply choose the ground-truth box with minimal area as its target. Hint They share the heads between different feature levels, not only making the detector parameter-efficient but also improving the detection performance. Instead of using the standard \\(exp(x)\\) , they make use of \\(exp(s_ix)\\) with a trainable scalar \\(s_i\\) to automatically adjust the base of the exponential function for feature level \\(P_i\\) , which slightly improves the detection performance.","title":"Multi-level Prediction with FPN for FCOS"},{"location":"av/detection/fcos/fcos/#center-ness-for-fcos","text":"After using multi-level prediction in FCOS, there is still a performance gap between FCOS and anchor-based detectors. They observed that it is due to a lot of low-quality predicted bounding boxes produced by locations far away from the center of an object. They add a single-layer branch, in parallel with the classification branch to predict the \"center-ness\" of a location. The center-ness depicts the normalized distance from the location to the center of the object that the location is responsible for.","title":"Center-ness for FCOS"},{"location":"av/detection/lane_net/lane_net/","text":"Towards End-to-End Lane Detection: an Instance Segmentation Approach Motivation Traditional Lane Detection Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations. Deep Learning Models More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances. Proposed Approach This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation. LaneNet Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances. H-Net \ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes. Method LaneNet The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network. Binary Segmentation \ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting. Instance Segmentation \ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized. Clustering The clustering is done by an iterative procedure. By setting \\(\\delta_d > 6\\delta_v\\) in the above loss, one can take a random lane embedding and threshold around it with a radius of \\(2 \\delta_v\\) to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding. Network Architecture \ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network Curve Fitting Using H-Net \ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial. Loss Function Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients. Network Architecture The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs. Results Dataset TuSimple lane dataset","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#towards-end-to-end-lane-detection-an-instance-segmentation-approach","text":"","title":"Towards End-to-End Lane Detection: an Instance Segmentation Approach"},{"location":"av/detection/lane_net/lane_net/#motivation","text":"","title":"Motivation"},{"location":"av/detection/lane_net/lane_net/#traditional-lane-detection","text":"Traditional lane detection methods rely on a combination of highly-specialized, hand-crafted features and heuristics, usually followed by post-processing techniques. Computationally expensive Prone to scalability due to road scene variations.","title":"Traditional Lane Detection"},{"location":"av/detection/lane_net/lane_net/#deep-learning-models","text":"More recent approaches leverage deep learning models, trained for pixel-wise lane segmentation, even when no markings are present in the image due to their big receptive field. Limited to detecting a pre-defined, fixed number of lanes, e.g. ego-lanes, and can not cope with lane changes. At a final stage, the generated binary lane segmentations still need to be disentangled into the different lane instances.","title":"Deep Learning Models"},{"location":"av/detection/lane_net/lane_net/#proposed-approach","text":"This paper propose to cast the lane detection problem as an instance segmentation problem \u2014 in which each lane forms its own instance \u2014 that can be trained end-to-end. To parametrize the segmented lane instances before fitting the lane, they further propose to apply a learned perspective transformation, conditioned on the image, in contrast to a fixed \u201cbird-eye-view\u201d transformation. By doing so, a lane fitting is robust against road plane changes, unlike existing approaches that rely on a fixed, predefined transformation.","title":"Proposed Approach"},{"location":"av/detection/lane_net/lane_net/#lanenet","text":"Inspired by the success of dense prediction networks in semantic segmentation and instance segmentation tasks, they design a branched, multi-task network for lane instance segmentation, consisting of a lane segmentation branch and a lance embedding branch that can be trained end-to-end. The lane segmentation branch has two output classes, background or lane, while the lane embedding branch further disentangles the segmented lane pixels into different lane instances.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#h-net","text":"\ud83d\udca1 Having estimated the lane instances, i.e., which pixels belong to which lane, as a final step we would like to convert each one of them into a parametric description. To this end, curve fitting algorithms have been widely used in the literature. Popular models are cubic polynomials splines clothoids To increase the quality of the fit while retaining computational efficiency, it is common to convert the image into a \u201cbird-eye view\u201d using a perspective transformation and perform the curve fitting there. In particular, the neural network takes as input the image and is optimized with a loss function that is tailored to the lane fitting problem. \ud83d\udca1 An inherent advantage of the proposed method is that the lane fitting is robust against road plane changes and is specifically optimized for better fitting the lanes.","title":"H-Net"},{"location":"av/detection/lane_net/lane_net/#method","text":"","title":"Method"},{"location":"av/detection/lane_net/lane_net/#lanenet_1","text":"The instance segmentation task consists of two parts, a segmentation and a clustering part. To increase performance, both in terms of speed and accuracy, these two parts are jointly trained in a multi-task network.","title":"LaneNet"},{"location":"av/detection/lane_net/lane_net/#binary-segmentation","text":"\ud83d\udca1 The segmentation branch of LaneNet is trained to output a binary segmentation map, indicating which pixels belong to a lane and which not. To construct the ground-truth segmentation map, we connect all ground-truth lane points together, forming a connected line per lane. Note that they draw these ground-truth lanes even through objects like occluding cars, or also in the absence of explicit visual lane segments, like dashed or faded lanes. This way, the network will learn to predict lane location even when they are occluded or in adverse circumstances. The segmentation network is trained with the standard cross-entropy loss function. Since the two classes (land/background) are highly unbalanced, they apply bounded inverse class weighting.","title":"Binary Segmentation"},{"location":"av/detection/lane_net/lane_net/#instance-segmentation","text":"\ud83d\udca1 Most popular detect-and-segment approaches are not ideal for lane instance segmentation, since bounding box detection is more suited for compact objects, which lanes are not. Therefore they use a one-shot method based on distance metric learning, which can easily be integrated with standard feed-forward networks and which is specifically designed for real-time applications. By using the clustering loss function, the instance embedding branch is trained to output an embedding for each lane pixel so that the distance between pixel embeddings belonging to the same lane is small, whereas the distance between pixel embeddings belonging to different lanes is maximized.","title":"Instance Segmentation"},{"location":"av/detection/lane_net/lane_net/#clustering","text":"The clustering is done by an iterative procedure. By setting \\(\\delta_d > 6\\delta_v\\) in the above loss, one can take a random lane embedding and threshold around it with a radius of \\(2 \\delta_v\\) to select all embeddings belong to the same lane. This is repeated until all lane embeddings are assigned to a lane. To avoid selecting an outlier to threshold around, we first use mean shift to shift closer to the cluster center and then do the thresholding.","title":"Clustering"},{"location":"av/detection/lane_net/lane_net/#network-architecture","text":"\ud83d\udca1 LaneNet\u2019s architecture is based on the encoder-decoder network ENet, which is consequently modified into a two-branched network. While the original ENet\u2019s encoder consists of three stages, LaneNet only shares the first two stages between the two branches, leaving stage 3 of the ENet encoder and the full ENet decoder as the backbone of each separate branch. The last layer of the segmentation branch outputs a one channel image, whereas the last layer of the embedding branch outputs a N-channel image, with N the embedding dimension. \ud83d\udca1 Each branch\u2019s loss term is equally weighted and back-propagated through the network","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#curve-fitting-using-h-net","text":"\ud83d\udca1 Fitting a polynomial through these pixels in the original image space is not ideal, as one has to resort to higher order polynomials to be able to cope with curved lanes. A frequently used solution to this problem is to project the image into a \u201cbird-eye view\u201d representation, in which lanes are parallel to each other and as such, curved lanes can be fitted with a 2nd or 3rd order polynomial.","title":"Curve Fitting Using H-Net"},{"location":"av/detection/lane_net/lane_net/#loss-function","text":"Since the lane fitting is done by using the closed-form solution of the least squares algorithm, the loss is differentiable. We use automatic differentiation to calculate the gradients.","title":"Loss Function"},{"location":"av/detection/lane_net/lane_net/#network-architecture_1","text":"The network architecture of H-Net is kept intentionally small and is constructed out of consecutive blocks of 3x3 convolutions, BNs and ReLUs.","title":"Network Architecture"},{"location":"av/detection/lane_net/lane_net/#results","text":"","title":"Results"},{"location":"av/detection/lane_net/lane_net/#dataset","text":"TuSimple lane dataset","title":"Dataset"},{"location":"av/detection/modules/attention/attention/","text":"Attention 101 A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images, etc.) and outputs another sequence of items (see [ NLP ]). Under the hood, the model is composed of an encoder and a decoder. . The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item. Context The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks. Hint Look at the hidden states for the encoder, and notice how the last hidden state is actually the context we pass along to the decoder. The decoder also maintains a hidden state that it passes from one time step to the next. Attention The context vector turned to be a bottleneck fo these types of models. It made it challenging for the models to deal with long sequences. A solution was proposed in Neural Machine Translation by Jointly Learning to Align and Translate Effective Approaches to Attention-based Neural Machine Translation They introduced and refined a technique called Attention , which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed. At time step 7, the attention mechanism enables the decoder to focus on the word \"\u00e9tudiant\" (\"student\" in french) before it generates the English translation. Note The ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention. An attention model differs from a classic sequence-to-sequence model in both encoding and decoding. Encoding The encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder. Decoding An attention decoder does an extra step before producing its output. In order to focus on the parts of the input that relevant to this decoding time step, the decoder does the following: Look at the set of encoder hidden states it received -- each encoder hidden state is most associated with a certain word in the input sequence. Given each hidden state a score (ignore how the scoring is done for now). Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. The scoring exercise is done at each time step on the decoder side. The attention decoder RNN takes in the embedding of <END> token, and an initial decoder hidden state. The RNN processes its inputs, producing an output and a new hidden state vector ( h4 ). The output is discarded. Attention Step: we use the encoder hidden states and the h4 vector to calculate a context vector ( C4 ) for this time step. We concatenate h4 and C4 into one vector. We pass this vector through a feed forward neural network (one trained jointly with the model). The output of the feed forward neural networks indicates the output word of this time step. Repeat for the next time steps. Reference Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)","title":"Attention 101"},{"location":"av/detection/modules/attention/attention/#attention-101","text":"A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images, etc.) and outputs another sequence of items (see [ NLP ]). Under the hood, the model is composed of an encoder and a decoder. . The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.","title":"Attention 101"},{"location":"av/detection/modules/attention/attention/#context","text":"The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks. Hint Look at the hidden states for the encoder, and notice how the last hidden state is actually the context we pass along to the decoder. The decoder also maintains a hidden state that it passes from one time step to the next.","title":"Context"},{"location":"av/detection/modules/attention/attention/#attention","text":"The context vector turned to be a bottleneck fo these types of models. It made it challenging for the models to deal with long sequences. A solution was proposed in Neural Machine Translation by Jointly Learning to Align and Translate Effective Approaches to Attention-based Neural Machine Translation They introduced and refined a technique called Attention , which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed. At time step 7, the attention mechanism enables the decoder to focus on the word \"\u00e9tudiant\" (\"student\" in french) before it generates the English translation. Note The ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention. An attention model differs from a classic sequence-to-sequence model in both encoding and decoding.","title":"Attention"},{"location":"av/detection/modules/attention/attention/#encoding","text":"The encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder.","title":"Encoding"},{"location":"av/detection/modules/attention/attention/#decoding","text":"An attention decoder does an extra step before producing its output. In order to focus on the parts of the input that relevant to this decoding time step, the decoder does the following: Look at the set of encoder hidden states it received -- each encoder hidden state is most associated with a certain word in the input sequence. Given each hidden state a score (ignore how the scoring is done for now). Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. The scoring exercise is done at each time step on the decoder side. The attention decoder RNN takes in the embedding of <END> token, and an initial decoder hidden state. The RNN processes its inputs, producing an output and a new hidden state vector ( h4 ). The output is discarded. Attention Step: we use the encoder hidden states and the h4 vector to calculate a context vector ( C4 ) for this time step. We concatenate h4 and C4 into one vector. We pass this vector through a feed forward neural network (one trained jointly with the model). The output of the feed forward neural networks indicates the output word of this time step. Repeat for the next time steps.","title":"Decoding"},{"location":"av/detection/modules/attention/attention/#reference","text":"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)","title":"Reference"},{"location":"av/detection/modules/mono_bev/mono_bev/","text":"Monocular BEV Perception Approaches IPM Lift-splat MLP Transformers View Transformation with Transformers Transformers are more suitable to perform the job of view transformation due to the global attention mechanism. There are two types of attention mechanisms in Transformers, self-attention in the Encoder and cross-attention in the decoder. The main difference between them is the query \\(Q\\) . - In self-attention, the \\(Q\\) , \\(K\\) , \\(V\\) inputs are the same. - In cross-attention, \\(Q\\) is in a different domain from that for \\(K\\) and \\(V\\) . Cross-attention Is All You Need Many of the recent advances in Transformers in CV actually only leverages the self-attention mechanism. They act as an enhancement to the backbone feature extractor. Warning Considering the difficulty in the deployment of the general Transformer architecture in resource-limited embedded systems typical on mass production vehicles, the incremental benefit of self-attention over the well-supported CNN can be hard to justify. Until we see some groundbreaking edge of self-attention over CNN, it would be wise choice to focus on CNN for industry applications. Cross-attention One pioneering study of applying cross-attention to computer vision is DETR. One of the most innovative parts of DETR is the cross-attention decoder based on a fixed number of slots called object queries. The content of the queries are also learned and do not have to be specified before training, except the number of the queries. Hint These queries can be viewed as a blank, preallocated template to hold object detection results, and the cross attention decoder does the work of filling in the blanks. This prompts the idea of using the cross-attention decoder for view transformation. - The input view is fed into a feature encoder (either self-attention based or CNN-based), and the encoded features serve as \\(K\\) and \\(V\\) . - The query \\(Q\\) in target view format can be learned and only need to be rasterized as a template. The values of \\(Q\\) can be learned jointly with the rest of the network. Reference Monocular BEV Perception with Transformers in Autonomous Driving","title":"Monocular BEV Perception"},{"location":"av/detection/modules/mono_bev/mono_bev/#monocular-bev-perception","text":"","title":"Monocular BEV Perception"},{"location":"av/detection/modules/mono_bev/mono_bev/#approaches","text":"IPM Lift-splat MLP Transformers","title":"Approaches"},{"location":"av/detection/modules/mono_bev/mono_bev/#view-transformation-with-transformers","text":"Transformers are more suitable to perform the job of view transformation due to the global attention mechanism. There are two types of attention mechanisms in Transformers, self-attention in the Encoder and cross-attention in the decoder. The main difference between them is the query \\(Q\\) . - In self-attention, the \\(Q\\) , \\(K\\) , \\(V\\) inputs are the same. - In cross-attention, \\(Q\\) is in a different domain from that for \\(K\\) and \\(V\\) .","title":"View Transformation with Transformers"},{"location":"av/detection/modules/mono_bev/mono_bev/#cross-attention-is-all-you-need","text":"Many of the recent advances in Transformers in CV actually only leverages the self-attention mechanism. They act as an enhancement to the backbone feature extractor. Warning Considering the difficulty in the deployment of the general Transformer architecture in resource-limited embedded systems typical on mass production vehicles, the incremental benefit of self-attention over the well-supported CNN can be hard to justify. Until we see some groundbreaking edge of self-attention over CNN, it would be wise choice to focus on CNN for industry applications.","title":"Cross-attention Is All You Need"},{"location":"av/detection/modules/mono_bev/mono_bev/#cross-attention","text":"One pioneering study of applying cross-attention to computer vision is DETR. One of the most innovative parts of DETR is the cross-attention decoder based on a fixed number of slots called object queries. The content of the queries are also learned and do not have to be specified before training, except the number of the queries. Hint These queries can be viewed as a blank, preallocated template to hold object detection results, and the cross attention decoder does the work of filling in the blanks. This prompts the idea of using the cross-attention decoder for view transformation. - The input view is fed into a feature encoder (either self-attention based or CNN-based), and the encoded features serve as \\(K\\) and \\(V\\) . - The query \\(Q\\) in target view format can be learned and only need to be rasterized as a template. The values of \\(Q\\) can be learned jointly with the rest of the network.","title":"Cross-attention"},{"location":"av/detection/modules/mono_bev/mono_bev/#reference","text":"Monocular BEV Perception with Transformers in Autonomous Driving","title":"Reference"},{"location":"av/detection/modules/nlp/nlp/","text":"Natural Language Processing with RNNs and Attention An Encoder-Decoder Network for Machine Translation Let's take a look at a simple neural machine translation model that will translate English sentences to French. In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note The French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it should have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that at inference time (after training), you will not have the target sentence to feed to the encoder. Instead, simply feed the decoder the word that it output at the previous step. Reference Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition","title":"Natural Language Processing with RNNs and Attention"},{"location":"av/detection/modules/nlp/nlp/#natural-language-processing-with-rnns-and-attention","text":"","title":"Natural Language Processing with RNNs and Attention"},{"location":"av/detection/modules/nlp/nlp/#an-encoder-decoder-network-for-machine-translation","text":"Let's take a look at a simple neural machine translation model that will translate English sentences to French. In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. Note The French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it should have output at the previous step (regardless of what it actually output). For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that at inference time (after training), you will not have the target sentence to feed to the encoder. Instead, simply feed the decoder the word that it output at the previous step.","title":"An Encoder-Decoder Network for Machine Translation"},{"location":"av/detection/modules/nlp/nlp/#reference","text":"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition","title":"Reference"},{"location":"av/detection/modules/rnn/rnn/","text":"Recurrent Neural Network Reference A friendly introduction to Recurrent Neural Networks","title":"Recurrent Neural Network"},{"location":"av/detection/modules/rnn/rnn/#recurrent-neural-network","text":"","title":"Recurrent Neural Network"},{"location":"av/detection/modules/rnn/rnn/#reference","text":"A friendly introduction to Recurrent Neural Networks","title":"Reference"},{"location":"av/detection/modules/transformer/transformer/","text":"Transformer A High-Level Look In a machine translation application, it would take a sentence in one language, and output its translation in another. Encoding The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2014 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number. The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers: Self-attention layer: a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Feed-forward network: the exact same feed-forward network is independently applied to each position. Decoding The decoder has both Self-attention layer and Feed Forward layer, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence. Encoding Bringing The Tensors Into The Picture The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512. The size of the list is a hyper-parameter we can set \u2014 basically it would be the length of the longest sentence in our training dataset. In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. Here we begin to see one key property of the Transformer: Note The word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. An encoder receives a list of vectors as input, and processes this list by passing these vectors into a self-attention layer, then into a feed-forward neural network ( the exact same network with each vector flowing through it separately ), then sends out the output upwards to the next encoder. Self-Attention Note Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing. As the model processes each word (each position in the input sequence), self-attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word. Self-Attention in Detail First Step The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector Note These vectors are created by multiplying the embedding by three matrices that we trained during the training process. Notice that these new vectors are smaller in dimension than the embedding vector. They don\u2019t have to be smaller, this is an architecture choice to make the computation of multi-headed attention (mostly) constant. Second Step The second step in calculating self-attention is to calculate a score. Note The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. The score determines how much focus to place on other parts of the input sentence as we encode at a certain position. So if we\u2019re processing the self-attention for the word in position #1: the first score would be the dot product of q1 and k1. the second score would be the dot product of q1 and k2. Third and Fourth Steps The third step is to divide the scores by the square root of the dimension of the key vectors used in the paper. This leads to having more stable gradients. There could be other possible values here, but this is the default. In the fourth step, we pass the result through a softmax operation. Softmax normalizes the scores so they\u2019are all positive and add up to 1. This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word. Fifth and Sixth Steps The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers). The sixth steps is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position. That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. Matrix Calculation of Self-Attention The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X , and multiplying it by the weight matrices we've trained ( \\(W^Q\\) , \\(W^K\\) , \\(W^V\\) ). The other steps can be condensed into this formula: Multi-Headed Attention Multi-headed attention improves the performance of the attention layer in two ways: It expands the model\u2019s ability to focus on different positions. In the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. It gives the attention layer multiple \u201crepresentation subspaces\u201d. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder) . Each of these is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices. This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2014 it\u2019s expecting a single matrix (a vector for each word). We need a way to condense these eight down into a single matrix. We concatenate the matrices then multiply them by an additional weights matrix \\(W^O\\) . Note The result would be the Z matrix that captures information from all the attention heads. We can send this matrix forward to the feed forward layers. Put everything together: Representing The Order of The Sequence Using Positional Encoding One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. Note The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vector once they\u2019re projected into Q/K/V vectors and during dot-product attention. There are some different possible methods for positional encoding. The Residuals One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer in each encoder has a residual connection around it, and is followed by a layer-normalization step. This goes for the sub-layers of the decoder as well. Decoding The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V . These are to be used by decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence. The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. The self-attention layers in the decoder operate in a slightly different way than the one in the encoder. Note In the decoder, the self-attention layer is only allowed to attend the earlier positions in the output sequence. This is done by masking future positions (setting them to -inf ) before the softmax step in the self-attention calculation. Encoder-Decoder Attention The \u201cEncoder-Decoder Attention\u201d layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack. The Final Linear and Softmax Layer The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let\u2019s assume that our model knows 10000 unique English words that it\u2019s learned from its training dataset. This would make the logits vector 10000 cells wide \u2014 each cell corresponding to the scope of a unique word. That is how we interpret the output of the model followed by the Linear layer. Reference Attention Is All You Need The Illustrated Transformer The Narrated Transformer Language Model The Annotated Transformer","title":"Transformer"},{"location":"av/detection/modules/transformer/transformer/#transformer","text":"","title":"Transformer"},{"location":"av/detection/modules/transformer/transformer/#a-high-level-look","text":"In a machine translation application, it would take a sentence in one language, and output its translation in another.","title":"A High-Level Look"},{"location":"av/detection/modules/transformer/transformer/#encoding","text":"The encoding component is a stack of encoders (the paper stacks six of them on top of each other \u2014 there\u2019s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number. The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers: Self-attention layer: a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. Feed-forward network: the exact same feed-forward network is independently applied to each position.","title":"Encoding"},{"location":"av/detection/modules/transformer/transformer/#decoding","text":"The decoder has both Self-attention layer and Feed Forward layer, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.","title":"Decoding"},{"location":"av/detection/modules/transformer/transformer/#encoding_1","text":"","title":"Encoding"},{"location":"av/detection/modules/transformer/transformer/#bringing-the-tensors-into-the-picture","text":"The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512. The size of the list is a hyper-parameter we can set \u2014 basically it would be the length of the longest sentence in our training dataset. In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. Here we begin to see one key property of the Transformer: Note The word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. An encoder receives a list of vectors as input, and processes this list by passing these vectors into a self-attention layer, then into a feed-forward neural network ( the exact same network with each vector flowing through it separately ), then sends out the output upwards to the next encoder.","title":"Bringing The Tensors Into The Picture"},{"location":"av/detection/modules/transformer/transformer/#self-attention","text":"Note Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing. As the model processes each word (each position in the input sequence), self-attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.","title":"Self-Attention"},{"location":"av/detection/modules/transformer/transformer/#self-attention-in-detail","text":"","title":"Self-Attention in Detail"},{"location":"av/detection/modules/transformer/transformer/#first-step","text":"The first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector Note These vectors are created by multiplying the embedding by three matrices that we trained during the training process. Notice that these new vectors are smaller in dimension than the embedding vector. They don\u2019t have to be smaller, this is an architecture choice to make the computation of multi-headed attention (mostly) constant.","title":"First Step"},{"location":"av/detection/modules/transformer/transformer/#second-step","text":"The second step in calculating self-attention is to calculate a score. Note The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. The score determines how much focus to place on other parts of the input sentence as we encode at a certain position. So if we\u2019re processing the self-attention for the word in position #1: the first score would be the dot product of q1 and k1. the second score would be the dot product of q1 and k2.","title":"Second Step"},{"location":"av/detection/modules/transformer/transformer/#third-and-fourth-steps","text":"The third step is to divide the scores by the square root of the dimension of the key vectors used in the paper. This leads to having more stable gradients. There could be other possible values here, but this is the default. In the fourth step, we pass the result through a softmax operation. Softmax normalizes the scores so they\u2019are all positive and add up to 1. This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word.","title":"Third and Fourth Steps"},{"location":"av/detection/modules/transformer/transformer/#fifth-and-sixth-steps","text":"The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers). The sixth steps is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position. That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network.","title":"Fifth and Sixth Steps"},{"location":"av/detection/modules/transformer/transformer/#matrix-calculation-of-self-attention","text":"The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X , and multiplying it by the weight matrices we've trained ( \\(W^Q\\) , \\(W^K\\) , \\(W^V\\) ). The other steps can be condensed into this formula:","title":"Matrix Calculation of Self-Attention"},{"location":"av/detection/modules/transformer/transformer/#multi-headed-attention","text":"Multi-headed attention improves the performance of the attention layer in two ways: It expands the model\u2019s ability to focus on different positions. In the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. It gives the attention layer multiple \u201crepresentation subspaces\u201d. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder) . Each of these is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices. This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices \u2014 it\u2019s expecting a single matrix (a vector for each word). We need a way to condense these eight down into a single matrix. We concatenate the matrices then multiply them by an additional weights matrix \\(W^O\\) . Note The result would be the Z matrix that captures information from all the attention heads. We can send this matrix forward to the feed forward layers. Put everything together:","title":"Multi-Headed Attention"},{"location":"av/detection/modules/transformer/transformer/#representing-the-order-of-the-sequence-using-positional-encoding","text":"One thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. Note The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vector once they\u2019re projected into Q/K/V vectors and during dot-product attention. There are some different possible methods for positional encoding.","title":"Representing The Order of The Sequence Using Positional Encoding"},{"location":"av/detection/modules/transformer/transformer/#the-residuals","text":"One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer in each encoder has a residual connection around it, and is followed by a layer-normalization step. This goes for the sub-layers of the decoder as well.","title":"The Residuals"},{"location":"av/detection/modules/transformer/transformer/#decoding_1","text":"The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V . These are to be used by decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence. The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. The self-attention layers in the decoder operate in a slightly different way than the one in the encoder. Note In the decoder, the self-attention layer is only allowed to attend the earlier positions in the output sequence. This is done by masking future positions (setting them to -inf ) before the softmax step in the self-attention calculation.","title":"Decoding"},{"location":"av/detection/modules/transformer/transformer/#encoder-decoder-attention","text":"The \u201cEncoder-Decoder Attention\u201d layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.","title":"Encoder-Decoder Attention"},{"location":"av/detection/modules/transformer/transformer/#the-final-linear-and-softmax-layer","text":"The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let\u2019s assume that our model knows 10000 unique English words that it\u2019s learned from its training dataset. This would make the logits vector 10000 cells wide \u2014 each cell corresponding to the scope of a unique word. That is how we interpret the output of the model followed by the Linear layer.","title":"The Final Linear and Softmax Layer"},{"location":"av/detection/modules/transformer/transformer/#reference","text":"Attention Is All You Need The Illustrated Transformer The Narrated Transformer Language Model The Annotated Transformer","title":"Reference"},{"location":"av/detection/modules/vision_transformer/vit/","text":"Vision Transformer The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. Classification Head Similar to BERT's [class] token, they prepend a learnable embedding to the sequence of embedded patches ( \\(\\textbf{z}^0_0 = \\textbf{x}_{class}\\) ), whose state at the output of the Transformer encoder ( \\(\\textbf{z}_L^0\\) ) serves as the image representation \\(\\textbf{y}\\) . Both during pre-training and fine-tuning, a classification head is attached to \\(\\textbf{z}_L^0\\) . Note The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time. Position Embedding Position embeddings are added to the patch embeddings to retain positional information. They use standard learnable 1D position embeddings .","title":"Vision Transformer"},{"location":"av/detection/modules/vision_transformer/vit/#vision-transformer","text":"The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection.","title":"Vision Transformer"},{"location":"av/detection/modules/vision_transformer/vit/#classification-head","text":"Similar to BERT's [class] token, they prepend a learnable embedding to the sequence of embedded patches ( \\(\\textbf{z}^0_0 = \\textbf{x}_{class}\\) ), whose state at the output of the Transformer encoder ( \\(\\textbf{z}_L^0\\) ) serves as the image representation \\(\\textbf{y}\\) . Both during pre-training and fine-tuning, a classification head is attached to \\(\\textbf{z}_L^0\\) . Note The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.","title":"Classification Head"},{"location":"av/detection/modules/vision_transformer/vit/#position-embedding","text":"Position embeddings are added to the patch embeddings to retain positional information. They use standard learnable 1D position embeddings .","title":"Position Embedding"},{"location":"av/detection/mtrans/mtrans/","text":"Multimodal Transformer for Automatic 3D Annotation and Object Detection Abstract To automate the annotation and facilitate the production of various customized datasets, they propose an end-to-end multimodal transformer (MTrans) autolabeler, which leverages both LiDAR scans and images to generate precise 3D box annotations from weak 2D bounding boxes. To alleviate the pervasive sparsity problem that hinders existing autolabelers, MTrans densifies the sparse point clouds by generating new 3D points based on 2D image information. With a multi-task design, MTrans segments the foreground/background, densifies LiDAR point clouds, and regresses 3D boxes simultaneously. Introduction Compared with 3D annotations, 2D bounding boxes are much easier to obtain. Hence, they investigate the generation of 3D annotations from weak 2D boxes. Even though the problem domain is narrowed down by the weak 2D annotations, it is still challenging to generate a 3D box if the points are too sparse, due to the ambiguity in orientation and scale. Note The sparsity problem remains a fundamental challenge for automatic annotations. Besides the sparsity issue, point clouds lack color information. Fortunately, images provide dense fine-grid RGB information, which is an effective complement to the sparse point clouds. Motivated by this, they investigate point cloud enrichment with image information. 3D LiDAR points can be projected onto the image, and therefore an image pixel's 3D coordinates can be estimated by referring to the context LiDAR points, based on their 2D spatial and semantic relationships. This way, new 3D points can be generated from 2D pixels, and hence densify the sparse point cloud. In order to leverage the dense image information to enrich sparse point clouds, they present the multimodal self-attention module, which efficiently extracts multimodal 2D and 3D features, modeling points' geometric and semantic relationships. To train MTrans, they introduce a multi-task design, so that it learns to generate new 3D points to reduce the sparsity of point clouds while predicting 3D boxes. Moreover, to exploit unlabelled data for the point generation task, a mask-and-predict self-supervision strategy can be carried out. MTrans for Automatic Annotation Data Preparation MTrans generates one 3D bounding box for each object in a weak 2D box.","title":"Multimodal Transformer for Automatic 3D Annotation and Object Detection"},{"location":"av/detection/mtrans/mtrans/#multimodal-transformer-for-automatic-3d-annotation-and-object-detection","text":"Abstract To automate the annotation and facilitate the production of various customized datasets, they propose an end-to-end multimodal transformer (MTrans) autolabeler, which leverages both LiDAR scans and images to generate precise 3D box annotations from weak 2D bounding boxes. To alleviate the pervasive sparsity problem that hinders existing autolabelers, MTrans densifies the sparse point clouds by generating new 3D points based on 2D image information. With a multi-task design, MTrans segments the foreground/background, densifies LiDAR point clouds, and regresses 3D boxes simultaneously.","title":"Multimodal Transformer for Automatic 3D Annotation and Object Detection"},{"location":"av/detection/mtrans/mtrans/#introduction","text":"Compared with 3D annotations, 2D bounding boxes are much easier to obtain. Hence, they investigate the generation of 3D annotations from weak 2D boxes. Even though the problem domain is narrowed down by the weak 2D annotations, it is still challenging to generate a 3D box if the points are too sparse, due to the ambiguity in orientation and scale. Note The sparsity problem remains a fundamental challenge for automatic annotations. Besides the sparsity issue, point clouds lack color information. Fortunately, images provide dense fine-grid RGB information, which is an effective complement to the sparse point clouds. Motivated by this, they investigate point cloud enrichment with image information. 3D LiDAR points can be projected onto the image, and therefore an image pixel's 3D coordinates can be estimated by referring to the context LiDAR points, based on their 2D spatial and semantic relationships. This way, new 3D points can be generated from 2D pixels, and hence densify the sparse point cloud. In order to leverage the dense image information to enrich sparse point clouds, they present the multimodal self-attention module, which efficiently extracts multimodal 2D and 3D features, modeling points' geometric and semantic relationships. To train MTrans, they introduce a multi-task design, so that it learns to generate new 3D points to reduce the sparsity of point clouds while predicting 3D boxes. Moreover, to exploit unlabelled data for the point generation task, a mask-and-predict self-supervision strategy can be carried out.","title":"Introduction"},{"location":"av/detection/mtrans/mtrans/#mtrans-for-automatic-annotation","text":"","title":"MTrans for Automatic Annotation"},{"location":"av/detection/mtrans/mtrans/#data-preparation","text":"MTrans generates one 3D bounding box for each object in a weak 2D box.","title":"Data Preparation"},{"location":"av/detection/nerf/nerf/","text":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"},{"location":"av/detection/nerf/nerf/#nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis","text":"","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"},{"location":"av/detection/persformer/notes/","text":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark Abstract Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. OpenLane: one of the first large-scale real-world 3D lane datasets, with high-quality annotation and scenario diversity. Introduction 2D Lane Detection With the prosperity of deep learning, lane detection algorithms in the 2D image space has achieved impressive results, where the task is formulated as a 2D segmentation problem given front view (perspective) image as input. Warning Such a framework to perform lane detection in the perspective view is not applicable for industry-level products where complicated scenarios dominate. BEV Perception Downstream modules as in planning and control often require the lane location to be in the form of the orthographic bird's eye view (BEV) instead of a front view representation. Representation in BEV is for better task alignment with interactive agents (vehicle, road marker, traffic light, etc.) in the environment and multi-modal compatibility with other sensors such as LiDAR and Radar. The conventional approaches to address such a demand are: either to simply project perspective lanes to ones in the BEV space, or more elegantly to cast perspective features to BEV by aid of camera in/extrinsic matrices. Spatial Transformer The latter solution is inspired by the spatial transformer network (STN) to generate a one-to-one correspondence from the image to BEV feature grids. By doing so, the quality of features in BEV depends solely on the quality of the corresponding feature in the front view. Warning The predictions using these outcome features are not adorable as the blemish of scale variance in the front view, which inherits from the camera's pinhole model, remains. Lane Line Height The height of lane lines has to be considered when we project perspective lanes into BEV space. The lanes would diverge/converge in case of uphill/downhill if the height is ignored, if the height is ignored, leading to improper action decisions as in the planning and control module. Previous literature inevitably hypothesize that lanes in the BEV space lie on a flat ground, i.e., the height of lanes is zero. The planar assumption does not hold true in most autonomous driving scenarios, e.g., uphill/downhill, bump, crush turn, etc. Failure Since the height information is unavailable on public benchmarks or complicated to acquire accurate ground truth, 3D lane detection is ill-posed. PersFormer A unified 2D/3D lane detection framework with Transformer. Transformer-based Spatial Feature Transformation Module They model the spatial feature transformation as a learning procedure that has an attention mechanism to capture the interaction both among local region in the front view feature and between two views (front view to BEV), consequently being able to generate a fine-grained BEV feature representation. the deformable attention mechanism is adopted to remarkably reduce the computational memory requirement. dynamically adjust keys through the cross-attention module to capture prominent feature among the local region. Unified 2D and 3D Lane Detection Tasks They further unify 2D and 3D lane detection tasks to benefit from the co-learning optimization. Related Work Vision Transformers in Bird's-Eye-View (BEV) Note Projecting features to BEV and performing downstream tasks in it has become more dominant and ensured better performance recently. Compared with conventional CNN structure, the cross attention scheme in Vision Transformers is naturally introduced to serve as a learnable transformation of features across different views. Instead of simply projecting features via IPM, the successful application of Transformers in view transformation has demonstrated great success in various domains, including 3D object detection, prediction, planning, etc. 3D Lane Detection 3D-LaneNet: End-to-End 3D Multiple Lane Detection (ICCV 2019) Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection (ECCV 2020) Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021) Methodology 2D/3D lane attributes: ordered set of coordinates categorical attribute indicating the type of lane each point has its visibility attribute Approach Overview The backbone takes the resized image as input and generates multi-scale front view features where EfficientNet is adopted. The Perspective Transformer takes the front view features as input and generates BEV features by the aid of camera intrinsic and extrinsic parameters. The lane detection heads are responsible for predicting 2D/3D coordinates as well as lane types. The 2D/3D detection heads are referred to as LaneATT and 3D-LaneNet with modification on the structure and anchor design. Proposed Perspective Transformer Note The general idea of Perspective Transformer is to use the coordinates transformation matrix from IPM as a reference to generate BEV feature representation, by attending related region (local context) in front view feature. On the assumption that the ground is flat and the camera parameters are given, a classical IPM approach calculates a set of coordinate mapping from front-view to BEV, where the BEV space is defined on the flat ground. Such a transformation enframes a strong prior on the attention unit in PersFormer to generate more representative BEV features. Tip The architecture of Perspective Transformer is inspired by popular approaches such as DETR , and consists of the self-attention module and cross-attention module.","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark"},{"location":"av/detection/persformer/notes/#persformer-3d-lane-detection-via-perspective-transformer-and-the-openlane-benchmark","text":"Abstract Previous work struggled in complex cases due to their simple designs of the spatial transformation between front view and bird's eye view (BEV) and the lack of a realistic dataset. PersFormer: an end-to-end monocular 3D lane detector with a novel Transformer-based spatial feature transformation module. OpenLane: one of the first large-scale real-world 3D lane datasets, with high-quality annotation and scenario diversity.","title":"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark"},{"location":"av/detection/persformer/notes/#introduction","text":"","title":"Introduction"},{"location":"av/detection/persformer/notes/#2d-lane-detection","text":"With the prosperity of deep learning, lane detection algorithms in the 2D image space has achieved impressive results, where the task is formulated as a 2D segmentation problem given front view (perspective) image as input. Warning Such a framework to perform lane detection in the perspective view is not applicable for industry-level products where complicated scenarios dominate.","title":"2D Lane Detection"},{"location":"av/detection/persformer/notes/#bev-perception","text":"Downstream modules as in planning and control often require the lane location to be in the form of the orthographic bird's eye view (BEV) instead of a front view representation. Representation in BEV is for better task alignment with interactive agents (vehicle, road marker, traffic light, etc.) in the environment and multi-modal compatibility with other sensors such as LiDAR and Radar. The conventional approaches to address such a demand are: either to simply project perspective lanes to ones in the BEV space, or more elegantly to cast perspective features to BEV by aid of camera in/extrinsic matrices.","title":"BEV Perception"},{"location":"av/detection/persformer/notes/#spatial-transformer","text":"The latter solution is inspired by the spatial transformer network (STN) to generate a one-to-one correspondence from the image to BEV feature grids. By doing so, the quality of features in BEV depends solely on the quality of the corresponding feature in the front view. Warning The predictions using these outcome features are not adorable as the blemish of scale variance in the front view, which inherits from the camera's pinhole model, remains.","title":"Spatial Transformer"},{"location":"av/detection/persformer/notes/#lane-line-height","text":"The height of lane lines has to be considered when we project perspective lanes into BEV space. The lanes would diverge/converge in case of uphill/downhill if the height is ignored, if the height is ignored, leading to improper action decisions as in the planning and control module. Previous literature inevitably hypothesize that lanes in the BEV space lie on a flat ground, i.e., the height of lanes is zero. The planar assumption does not hold true in most autonomous driving scenarios, e.g., uphill/downhill, bump, crush turn, etc. Failure Since the height information is unavailable on public benchmarks or complicated to acquire accurate ground truth, 3D lane detection is ill-posed.","title":"Lane Line Height"},{"location":"av/detection/persformer/notes/#persformer","text":"A unified 2D/3D lane detection framework with Transformer.","title":"PersFormer"},{"location":"av/detection/persformer/notes/#transformer-based-spatial-feature-transformation-module","text":"They model the spatial feature transformation as a learning procedure that has an attention mechanism to capture the interaction both among local region in the front view feature and between two views (front view to BEV), consequently being able to generate a fine-grained BEV feature representation. the deformable attention mechanism is adopted to remarkably reduce the computational memory requirement. dynamically adjust keys through the cross-attention module to capture prominent feature among the local region.","title":"Transformer-based Spatial Feature Transformation Module"},{"location":"av/detection/persformer/notes/#unified-2d-and-3d-lane-detection-tasks","text":"They further unify 2D and 3D lane detection tasks to benefit from the co-learning optimization.","title":"Unified 2D and 3D Lane Detection Tasks"},{"location":"av/detection/persformer/notes/#related-work","text":"","title":"Related Work"},{"location":"av/detection/persformer/notes/#vision-transformers-in-birds-eye-view-bev","text":"Note Projecting features to BEV and performing downstream tasks in it has become more dominant and ensured better performance recently. Compared with conventional CNN structure, the cross attention scheme in Vision Transformers is naturally introduced to serve as a learnable transformation of features across different views. Instead of simply projecting features via IPM, the successful application of Transformers in view transformation has demonstrated great success in various domains, including 3D object detection, prediction, planning, etc.","title":"Vision Transformers in Bird's-Eye-View (BEV)"},{"location":"av/detection/persformer/notes/#3d-lane-detection","text":"3D-LaneNet: End-to-End 3D Multiple Lane Detection (ICCV 2019) Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection (ECCV 2020) Keep your Eyes on the Lane: Real-time Attention-guided Lane Detection (CVPR 2021)","title":"3D Lane Detection"},{"location":"av/detection/persformer/notes/#methodology","text":"2D/3D lane attributes: ordered set of coordinates categorical attribute indicating the type of lane each point has its visibility attribute","title":"Methodology"},{"location":"av/detection/persformer/notes/#approach-overview","text":"The backbone takes the resized image as input and generates multi-scale front view features where EfficientNet is adopted. The Perspective Transformer takes the front view features as input and generates BEV features by the aid of camera intrinsic and extrinsic parameters. The lane detection heads are responsible for predicting 2D/3D coordinates as well as lane types. The 2D/3D detection heads are referred to as LaneATT and 3D-LaneNet with modification on the structure and anchor design.","title":"Approach Overview"},{"location":"av/detection/persformer/notes/#proposed-perspective-transformer","text":"Note The general idea of Perspective Transformer is to use the coordinates transformation matrix from IPM as a reference to generate BEV feature representation, by attending related region (local context) in front view feature. On the assumption that the ground is flat and the camera parameters are given, a classical IPM approach calculates a set of coordinate mapping from front-view to BEV, where the BEV space is defined on the flat ground. Such a transformation enframes a strong prior on the attention unit in PersFormer to generate more representative BEV features. Tip The architecture of Perspective Transformer is inspired by popular approaches such as DETR , and consists of the self-attention module and cross-attention module.","title":"Proposed Perspective Transformer"},{"location":"av/detection/vision_transformer/vit/","text":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Abstract In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. They show that the reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Vision Transformer (ViT) can attain excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. Introduction Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest pssible modifications. Note They split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. Related Work Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Several approximations have been tried: Apply the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions. Sparse Transformers employ scalable approximations to global self attention in order to be applicable to images. Apply self attention in blocks of varying sizes. ... Warning Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. A very similar work is On the Relationship between Self-Attention and Convolutional Layers (ICLR 2020) . The main differences are this work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. this work uses a larger patch size, which makes the model applicable to medium-resolution images as well. Method ViT follow the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures - and their efficient implementations - can be used almost out of the box. Vision Transformer (ViT) The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection. Classification Head Similar to BERT's [class] token, they prepend a learnable embedding to the sequence of embedded patches ( \\(\\textbf{z}^0_0 = \\textbf{x}_{class}\\) ), whose state at the output of the Transformer encoder ( \\(\\textbf{z}_L^0\\) ) serves as the image representation \\(\\textbf{y}\\) . Both during pre-training and fine-tuning, a classification head is attached to \\(\\textbf{z}_L^0\\) . Note The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time. Position Embedding Position embeddings are added to the patch embeddings to retain positional information. They use standard learnable 1D position embeddings . Inductive Bias Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. Hybrid Architecture As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN. In this hybrid model, the patch embedding projection \\(\\textbf{E}\\) is applied to patches extracted from a CNN feature map.","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"location":"av/detection/vision_transformer/vit/#an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale","text":"Abstract In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. They show that the reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. Vision Transformer (ViT) can attain excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"location":"av/detection/vision_transformer/vit/#introduction","text":"Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest pssible modifications. Note They split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application.","title":"Introduction"},{"location":"av/detection/vision_transformer/vit/#related-work","text":"Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Several approximations have been tried: Apply the self-attention only in local neighborhoods for each query pixel instead of globally. Such local multi-head dot-product self attention blocks can completely replace convolutions. Sparse Transformers employ scalable approximations to global self attention in order to be applicable to images. Apply self attention in blocks of varying sizes. ... Warning Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators. A very similar work is On the Relationship between Self-Attention and Convolutional Layers (ICLR 2020) . The main differences are this work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. this work uses a larger patch size, which makes the model applicable to medium-resolution images as well.","title":"Related Work"},{"location":"av/detection/vision_transformer/vit/#method","text":"ViT follow the original Transformer as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures - and their efficient implementations - can be used almost out of the box.","title":"Method"},{"location":"av/detection/vision_transformer/vit/#vision-transformer-vit","text":"The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, they reshape the image \\(x \\in \\mathbb{R}^{H \\times W \\times C}\\) into a sequence of flattened 2D patches \\(N \\times \\textbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\) , where \\((H, W)\\) is the resolution of the original image, \\(C\\) is the number of channels, \\((P, P)\\) is the resolution of each image patch, and \\(N = HW / P^2\\) is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. Note The Transformer uses constant latent vector size \\(D\\) through all of its layers, so they flatten the patches and map to \\(D\\) dimensions with a trainable linear projection.","title":"Vision Transformer (ViT)"},{"location":"av/detection/vision_transformer/vit/#classification-head","text":"Similar to BERT's [class] token, they prepend a learnable embedding to the sequence of embedded patches ( \\(\\textbf{z}^0_0 = \\textbf{x}_{class}\\) ), whose state at the output of the Transformer encoder ( \\(\\textbf{z}_L^0\\) ) serves as the image representation \\(\\textbf{y}\\) . Both during pre-training and fine-tuning, a classification head is attached to \\(\\textbf{z}_L^0\\) . Note The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.","title":"Classification Head"},{"location":"av/detection/vision_transformer/vit/#position-embedding","text":"Position embeddings are added to the patch embeddings to retain positional information. They use standard learnable 1D position embeddings .","title":"Position Embedding"},{"location":"av/detection/vision_transformer/vit/#inductive-bias","text":"Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global.","title":"Inductive Bias"},{"location":"av/detection/vision_transformer/vit/#hybrid-architecture","text":"As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN. In this hybrid model, the patch embedding projection \\(\\textbf{E}\\) is applied to patches extracted from a CNN feature map.","title":"Hybrid Architecture"},{"location":"av/detection/yolo_v1/yolo_v1/","text":"You Only Look Once: Unified, Real-Time Object Detection Abstract Prior work on object detection repurposes classifiers to perform detection. Instead, they frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. The unified architecture is extremely fast. Introduction Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image. More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene. These complex pipelines are slow and hard to optimize because each individual component must be trained separately. They reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. This unified model has several benefits over traditional methods of object detection. YOLO is extremely fast. YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. YOLO learns generalizable representations of objects. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs. Warning YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones. Unified Detection They unify the separate components of object detection into a single neural network. Their network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means the network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision. Grid The system divides the input image into an \\(S \\times S\\) grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. Confidence Score Each grid cell predicts \\(B\\) bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally, the confidence score is defined as \\(P_{Object} * IOU^{gt}_{pred}\\) . Predictions Each bounding box consists of 5 predictions: \\(x, y, w, h\\) . The \\((x, y)\\) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. The confidence prediction represents the IOU between the predicted box and any ground truth box. Classification Each grid cell also predicts \\(C\\) conditional class probabilities, \\(P(Class_i | Object)\\) . These probabilities are conditioned on the grid cell containing an object. They only predict one set of class probabilities per grid cell, regardless of the number of boxes \\(B\\) . At test time, they multiply the conditional class probabilities and the individual box confidence predictions, which gives class-specific scores for each box. Network Design Their network architecture is inspired by the GoogLeNet model for image classification. Their network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, they simply use \\(1 x 1\\) reduction layers followed by \\(3 x 3\\) convolutional layers. Training They pretrain convolutional layers on the ImageNet 1000-class competition dataset. They use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. They use the Darknet framework for all training and inference. They then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance. They add four convolutional layers and two fully connected layers with randomly initialized weights. Note They use a linear activation function fo the final layer and all other layers use the leaky rectified linear activation. Sum-squared error They optimize for sum-squared error in the output of the model. They use sum-squared error because it is easy to optimize, however it does not perfectly align with the goal of maximizing average precision. Warning It weights localization error equally with classification error which may not be ideal. Imbalance Class In every image many grid cells do not contain any object. It pushes the confidence scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on. To remedy this, they increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don't contain objects. Box Size Sum-squared error also equally weights errors in large boxes and small boxes. One error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this, they predict the squared root of the bounding box width and height instead of the width and height directly. Association YOLO predicts multiple bounding boxes per grid cell. At training time, only one bounding box should be responsible for each object. Note They assign one predictor to be \"responsible\" for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets getter at predicting certain sizes, aspect ratios, or classes of object, improving overall recall. Training Schedule Batch size of 64, a momentum of 0.9 and a decay of 0.0005 Learning rate: 1e-3, 1e-2, 1e-4. Hint If they start at a high learning rate the model often diverges due to unstable gradients. To avoid overfitting they use dropout and extensive data augmentation. Limitations of YOLO Warning YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that the model can prediction. The model struggles with small objects that appear in groups. Warning The main source of error is incorrect localizations. While they train on a loss function that approximates detection performance, their loss function treats errors the same in small bounding boxes versus large bounding boxes. Comparison to Other Detection Systems R-CNN YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. YOLO puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. YOLO proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search.","title":"You Only Look Once: Unified, Real-Time Object Detection"},{"location":"av/detection/yolo_v1/yolo_v1/#you-only-look-once-unified-real-time-object-detection","text":"Abstract Prior work on object detection repurposes classifiers to perform detection. Instead, they frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. The unified architecture is extremely fast.","title":"You Only Look Once: Unified, Real-Time Object Detection"},{"location":"av/detection/yolo_v1/yolo_v1/#introduction","text":"Current detection systems repurpose classifiers to perform detection. To detect an object, these systems take a classifier for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classifier is run at evenly spaced locations over the entire image. More recent approaches like R-CNN use region proposal methods to first generate potential bounding boxes in an image and then run a classifier on these proposed boxes. After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene. These complex pipelines are slow and hard to optimize because each individual component must be trained separately. They reframe object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. This unified model has several benefits over traditional methods of object detection. YOLO is extremely fast. YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. YOLO learns generalizable representations of objects. Since YOLO is highly generalizable it is less likely to break down when applied to new domains or unexpected inputs. Warning YOLO still lags behind state-of-the-art detection systems in accuracy. While it can quickly identify objects in images it struggles to precisely localize some objects, especially small ones.","title":"Introduction"},{"location":"av/detection/yolo_v1/yolo_v1/#unified-detection","text":"They unify the separate components of object detection into a single neural network. Their network uses features from the entire image to predict each bounding box. It also predicts all bounding boxes across all classes for an image simultaneously. This means the network reasons globally about the full image and all the objects in the image. The YOLO design enables end-to-end training and real-time speeds while maintaining high average precision. Grid The system divides the input image into an \\(S \\times S\\) grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object. Confidence Score Each grid cell predicts \\(B\\) bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and also how accurate it thinks the box is that it predicts. Formally, the confidence score is defined as \\(P_{Object} * IOU^{gt}_{pred}\\) . Predictions Each bounding box consists of 5 predictions: \\(x, y, w, h\\) . The \\((x, y)\\) coordinates represent the center of the box relative to the bounds of the grid cell. The width and height are predicted relative to the whole image. The confidence prediction represents the IOU between the predicted box and any ground truth box. Classification Each grid cell also predicts \\(C\\) conditional class probabilities, \\(P(Class_i | Object)\\) . These probabilities are conditioned on the grid cell containing an object. They only predict one set of class probabilities per grid cell, regardless of the number of boxes \\(B\\) . At test time, they multiply the conditional class probabilities and the individual box confidence predictions, which gives class-specific scores for each box.","title":"Unified Detection"},{"location":"av/detection/yolo_v1/yolo_v1/#network-design","text":"Their network architecture is inspired by the GoogLeNet model for image classification. Their network has 24 convolutional layers followed by 2 fully connected layers. Instead of the inception modules used by GoogLeNet, they simply use \\(1 x 1\\) reduction layers followed by \\(3 x 3\\) convolutional layers.","title":"Network Design"},{"location":"av/detection/yolo_v1/yolo_v1/#training","text":"They pretrain convolutional layers on the ImageNet 1000-class competition dataset. They use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. They use the Darknet framework for all training and inference. They then convert the model to perform detection. Ren et al. show that adding both convolutional and connected layers to pretrained networks can improve performance. They add four convolutional layers and two fully connected layers with randomly initialized weights. Note They use a linear activation function fo the final layer and all other layers use the leaky rectified linear activation.","title":"Training"},{"location":"av/detection/yolo_v1/yolo_v1/#sum-squared-error","text":"They optimize for sum-squared error in the output of the model. They use sum-squared error because it is easy to optimize, however it does not perfectly align with the goal of maximizing average precision. Warning It weights localization error equally with classification error which may not be ideal. Imbalance Class In every image many grid cells do not contain any object. It pushes the confidence scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on. To remedy this, they increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don't contain objects. Box Size Sum-squared error also equally weights errors in large boxes and small boxes. One error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this, they predict the squared root of the bounding box width and height instead of the width and height directly. Association YOLO predicts multiple bounding boxes per grid cell. At training time, only one bounding box should be responsible for each object. Note They assign one predictor to be \"responsible\" for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets getter at predicting certain sizes, aspect ratios, or classes of object, improving overall recall.","title":"Sum-squared error"},{"location":"av/detection/yolo_v1/yolo_v1/#training-schedule","text":"Batch size of 64, a momentum of 0.9 and a decay of 0.0005 Learning rate: 1e-3, 1e-2, 1e-4. Hint If they start at a high learning rate the model often diverges due to unstable gradients. To avoid overfitting they use dropout and extensive data augmentation.","title":"Training Schedule"},{"location":"av/detection/yolo_v1/yolo_v1/#limitations-of-yolo","text":"Warning YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that the model can prediction. The model struggles with small objects that appear in groups. Warning The main source of error is incorrect localizations. While they train on a loss function that approximates detection performance, their loss function treats errors the same in small bounding boxes versus large bounding boxes.","title":"Limitations of YOLO"},{"location":"av/detection/yolo_v1/yolo_v1/#comparison-to-other-detection-systems","text":"","title":"Comparison to Other Detection Systems"},{"location":"av/detection/yolo_v1/yolo_v1/#r-cnn","text":"YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. YOLO puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. YOLO proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search.","title":"R-CNN"},{"location":"av/detection/yolo_v2/yolo_v2/","text":"YOLO9000: Better, Faster, Stronger Abstract They introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. They propose various improvements to the YOLO detection method, both novel and drawn from prior work. Using a novel, multi-scale training method, the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. They propose a method to jointly train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. The joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. Introduction Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). They propose a new method to harness the large amount of classification data and use it to expand the scope of current detection systems. They uses a hierarchical view of object classification that allows to combine distinct datasets together. Note Their method leverages labeled detection images to learn to precisely localize objects while it uses classification images to increase its vocabulary and robustness. Better YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared with Fast R-CNN shows that YOLO makes a significant number of localization errors. Futhermore, YOLO has relatively low recall compared to region proposal-based methods. YOLOv2 mainly improves recall and localization while maintaining classification accuracy. With YOLOv2 they want a more accurate detector that is still fast. Instead of scaling up the network, they simplify the network and then make the representation easier to learn. They pool a variety of ideas from past work with their own novel concepts to improve YOLO's performance. Batch Normalization Batch normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization. By adding batch normalization on all of the convolutional layers in YOLO, they got more than 2% improvement in mAP. Batch normalization also regularize the model. With batch normalization, they can remove dropout from the model without overfitting. High Resolution Classifier The original YOLO trains the classifier network at 224 x 224 and increases the resolution to 448 for detection. Warning This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution. For YOLOv2 they first fine tune the classification network at the full 448 x 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. They then fine tune the resulting network on detection. This high resolution classification network gives an increase of almost 4% mAP. Convolutional With Anchor Boxes Instead of predicting coordinates directly, Faster R-CNN predicts bounding boxes using hand-picked priors. Using only convolutional layers, the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Hint Since the prediction layer is convolutional, the RPN predicts these offsets at every location ina feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn. Using anchor boxes they get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes, the model predicts more than a thousand. Even though the mAP decreases, the increase in recall means that the model has more room to improve. Dimension Clusters Instead of choosing priors by hand, they run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance, larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for the distance metric they use \\[d(box, centroid) = 1 - IOU(box, centroid)\\] The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes. Note Using k-means to generate the bounding box starts the model off with a better representation and makes the task easier to learn. Direct Location Prediction When using anchor boxes with YOLO, a second issue is model instability, especially during early iterations. Most of the instability comes from predicting the \\((x, y)\\) locations for the box. RPN predicts values \\(t_x\\) and \\(t_y\\) and the \\((x, y)\\) center coordinates are calculated as: Warning This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets. Instead of predicting offsets, they follow the approach of YOLOv1 and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. They use a logistic activation to constrain the network's predictions to fall in this range. The network predicts 5 bounding boxes at each cell in the output feature map. The network predicts 5 coordinates for each bounding box, \\(t_x\\) , \\(t_y\\) , \\(t_w\\) , and \\(t_o\\) . If the cell is offset from the top left corner of the image by \\((c_x, c_y)\\) and the bounding box prior has width and height \\(p_w\\) , \\(p_h\\) , then the predictions correspond to: Since they constrain the location prediction, the parametrization is easier to learn, making the network more stable. Fine-Grained Features Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. They take a different approach, simply adding a passthrough layer that brings features from an earlier layer at 26 x 26 resolution. The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet.","title":"YOLO9000: Better, Faster, Stronger"},{"location":"av/detection/yolo_v2/yolo_v2/#yolo9000-better-faster-stronger","text":"Abstract They introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. They propose various improvements to the YOLO detection method, both novel and drawn from prior work. Using a novel, multi-scale training method, the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. They propose a method to jointly train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. The joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data.","title":"YOLO9000: Better, Faster, Stronger"},{"location":"av/detection/yolo_v2/yolo_v2/#introduction","text":"Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). They propose a new method to harness the large amount of classification data and use it to expand the scope of current detection systems. They uses a hierarchical view of object classification that allows to combine distinct datasets together. Note Their method leverages labeled detection images to learn to precisely localize objects while it uses classification images to increase its vocabulary and robustness.","title":"Introduction"},{"location":"av/detection/yolo_v2/yolo_v2/#better","text":"YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared with Fast R-CNN shows that YOLO makes a significant number of localization errors. Futhermore, YOLO has relatively low recall compared to region proposal-based methods. YOLOv2 mainly improves recall and localization while maintaining classification accuracy. With YOLOv2 they want a more accurate detector that is still fast. Instead of scaling up the network, they simplify the network and then make the representation easier to learn. They pool a variety of ideas from past work with their own novel concepts to improve YOLO's performance. Batch Normalization Batch normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization. By adding batch normalization on all of the convolutional layers in YOLO, they got more than 2% improvement in mAP. Batch normalization also regularize the model. With batch normalization, they can remove dropout from the model without overfitting. High Resolution Classifier The original YOLO trains the classifier network at 224 x 224 and increases the resolution to 448 for detection. Warning This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution. For YOLOv2 they first fine tune the classification network at the full 448 x 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. They then fine tune the resulting network on detection. This high resolution classification network gives an increase of almost 4% mAP. Convolutional With Anchor Boxes Instead of predicting coordinates directly, Faster R-CNN predicts bounding boxes using hand-picked priors. Using only convolutional layers, the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Hint Since the prediction layer is convolutional, the RPN predicts these offsets at every location ina feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn. Using anchor boxes they get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes, the model predicts more than a thousand. Even though the mAP decreases, the increase in recall means that the model has more room to improve. Dimension Clusters Instead of choosing priors by hand, they run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance, larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for the distance metric they use \\[d(box, centroid) = 1 - IOU(box, centroid)\\] The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes. Note Using k-means to generate the bounding box starts the model off with a better representation and makes the task easier to learn. Direct Location Prediction When using anchor boxes with YOLO, a second issue is model instability, especially during early iterations. Most of the instability comes from predicting the \\((x, y)\\) locations for the box. RPN predicts values \\(t_x\\) and \\(t_y\\) and the \\((x, y)\\) center coordinates are calculated as: Warning This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets. Instead of predicting offsets, they follow the approach of YOLOv1 and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. They use a logistic activation to constrain the network's predictions to fall in this range. The network predicts 5 bounding boxes at each cell in the output feature map. The network predicts 5 coordinates for each bounding box, \\(t_x\\) , \\(t_y\\) , \\(t_w\\) , and \\(t_o\\) . If the cell is offset from the top left corner of the image by \\((c_x, c_y)\\) and the bounding box prior has width and height \\(p_w\\) , \\(p_h\\) , then the predictions correspond to: Since they constrain the location prediction, the parametrization is easier to learn, making the network more stable. Fine-Grained Features Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. They take a different approach, simply adding a passthrough layer that brings features from an earlier layer at 26 x 26 resolution. The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet.","title":"Better"},{"location":"av/detection/yolo_v3/yolo_v3/","text":"YOLOv3: An Incremental Improvement Abstract A bunch of little design changes to make it better. It's a little bigger than last time but more accurate. The Deal Bounding Box Prediction Following YOLOv2, YOLOv3 predicts bounding boxes using dimension clusters as anchor boxes. The network predicts 4 coordinates for each bounding box, \\(t_x\\) , \\(t_y\\) , \\(t_w\\) , \\(t_h\\) . If the cell is offset from the top left corner of the image by \\((c_x, c_y)\\) and the bounding box prior has width and height \\(p_w\\) , \\(p_h\\) , then the predictions correspond to: YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold they ignore the prediction, following Faster-RCNN (they use the threshold of 0.5). Hint Unlike Faster-RCNN, they only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness. Class Prediction Each box predicts the classes the bounding box may contain using multilabel classification. They do not use a softmax as they found it is unnecessary for good performance, instead they simply use independent logistic classifiers. During training they use binary cross-entropy loss for the class predictions. Predictions Across Scales YOLOv3 predicts boxes at 3 different scales. Their system extract features from those scales using a similar concept to feature pyramid networks (FPN). From their base feature extractor they add several convolutional layers. The last of these predicts a 3D tensor encoding bounding box, objectness, and class predictions. Next they take the feature map from 2 layers previous and upsample it by 2x. They also take a feature map from earlier in the network and merge it with their upsampled features using concatenation. They then add a few more convolutional layers to process this combined feature map, and eventually predict a similar tensor, although now twice the size. They perform the same design one more time to predict boxes for the final scale. Feature Extractor The new network is a hybrid approach between the network used in YOLOv2 (Darknet-19) and that new fangled residual network stuff. The network uses successive 3 x 3 and 1 x 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so they call it Darknet-53. Things They Tried That Didn't Work Focal loss. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Dual IoU thresholds and truth assignment. Faster-RCNN uses two IoU thresholds during training. If a prediction overlaps the ground truth by 0.7 it is as a positive example, by [0.3 - 0.7] it is ignored, less than 0.3 for all ground truth objects it is a negative example.","title":"YOLOv3: An Incremental Improvement"},{"location":"av/detection/yolo_v3/yolo_v3/#yolov3-an-incremental-improvement","text":"Abstract A bunch of little design changes to make it better. It's a little bigger than last time but more accurate.","title":"YOLOv3: An Incremental Improvement"},{"location":"av/detection/yolo_v3/yolo_v3/#the-deal","text":"","title":"The Deal"},{"location":"av/detection/yolo_v3/yolo_v3/#bounding-box-prediction","text":"Following YOLOv2, YOLOv3 predicts bounding boxes using dimension clusters as anchor boxes. The network predicts 4 coordinates for each bounding box, \\(t_x\\) , \\(t_y\\) , \\(t_w\\) , \\(t_h\\) . If the cell is offset from the top left corner of the image by \\((c_x, c_y)\\) and the bounding box prior has width and height \\(p_w\\) , \\(p_h\\) , then the predictions correspond to: YOLOv3 predicts an objectness score for each bounding box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than any other bounding box prior. If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold they ignore the prediction, following Faster-RCNN (they use the threshold of 0.5). Hint Unlike Faster-RCNN, they only assigns one bounding box prior for each ground truth object. If a bounding box prior is not assigned to a ground truth object it incurs no loss for coordinate or class predictions, only objectness.","title":"Bounding Box Prediction"},{"location":"av/detection/yolo_v3/yolo_v3/#class-prediction","text":"Each box predicts the classes the bounding box may contain using multilabel classification. They do not use a softmax as they found it is unnecessary for good performance, instead they simply use independent logistic classifiers. During training they use binary cross-entropy loss for the class predictions.","title":"Class Prediction"},{"location":"av/detection/yolo_v3/yolo_v3/#predictions-across-scales","text":"YOLOv3 predicts boxes at 3 different scales. Their system extract features from those scales using a similar concept to feature pyramid networks (FPN). From their base feature extractor they add several convolutional layers. The last of these predicts a 3D tensor encoding bounding box, objectness, and class predictions. Next they take the feature map from 2 layers previous and upsample it by 2x. They also take a feature map from earlier in the network and merge it with their upsampled features using concatenation. They then add a few more convolutional layers to process this combined feature map, and eventually predict a similar tensor, although now twice the size. They perform the same design one more time to predict boxes for the final scale.","title":"Predictions Across Scales"},{"location":"av/detection/yolo_v3/yolo_v3/#feature-extractor","text":"The new network is a hybrid approach between the network used in YOLOv2 (Darknet-19) and that new fangled residual network stuff. The network uses successive 3 x 3 and 1 x 1 convolutional layers but now has some shortcut connections as well and is significantly larger. It has 53 convolutional layers so they call it Darknet-53.","title":"Feature Extractor"},{"location":"av/detection/yolo_v3/yolo_v3/#things-they-tried-that-didnt-work","text":"Focal loss. YOLOv3 may already be robust to the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Dual IoU thresholds and truth assignment. Faster-RCNN uses two IoU thresholds during training. If a prediction overlaps the ground truth by 0.7 it is as a positive example, by [0.3 - 0.7] it is ignored, less than 0.3 for all ground truth objects it is a negative example.","title":"Things They Tried That Didn't Work"},{"location":"av/detection/yolo_v4/yolo_v4/","text":"YOLOv4: Optimal Speed and Accuracy of Object Detection Abstract There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. Introduction The main goal of this work is designing a fast operating speed of an object detector in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). Related Work Object Detection Models A modern detector is usually composed of two parts, a backbone which is pre-trained on ImageNet and a head which is used to predict classes and bounding boxes of objects. Object detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. They call it the neck of an object detector. Bag of Freebies They call methods that only change the training strategy or only increase the training cost as \"bag of freebies\". Data Augmentation Bag of Specials For those plugin modules and post-processing methods that only increase the inference cost by a small amount but can significantly improve the accuracy of object detection, they call them \"bag of specials\". Methodology Note The basic aim is fast operating speed of neural network, in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). They present two options of real-time neural networks: For GPU, they use a small number of groups (1 - 8) in convolutional layers: CSPResNeXt50/CSPDarknet53. For VPU, they use grouped-convolution, but they retrain from using Squeeze-and-excitement (SE) blocks -- specifically this includes the following models: EfficientNet-lite/MixNet/GhostNet/MobileNetV3. Selection of Architecture The objective is to find the optimal balance among the input network resolution, the convolutional layer number, the parameter number (filter_size^2 * filters * channel/groups), and the number of layer outputs (filters). The next objective is to select additional blocks for increasing the receptive field and the best method of parameter aggregation from different backbone levels for different detector levels: e.g. FPN, PAN, ASFF, BiFPN Hint A reference model which is optimal for classification is not always optimal for a detector. In contrast to the classifier, the detector requires the following: Higher input network size (resolution) - for detecting multiple small-sized objects. More layers - for a higher receptive field to cover the increased size of input network. More parameters - for greater capacity of a model to detect multiple objects of different sizes in a single image. The influence of the receptive field with different sizes is summarized as follows: Up to the object size - allows viewing the entire object. Up to network size - allows viewing the context around the object. Exceeding the network size - increases the number of connections between the image point and the final activation. They add the SPP block over the CSPDarknet53, since it significantly increases the receptive field, separates out the most significant context features and causes almost no reduction of the network operation speed. They use PANet as the method of parameter aggregation from different backbone levels for different detector levels, instead of the FPN used in YOLOv3. Note They choose CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv3 (anchor based) head as the architecture of YOLOv4. Selection of BoF and BoS For improving the object detection training, a CNN usually uses the following: Activations : ReLU, leaky-ReLU, parametric-ReLU, ReLU6, SELU, Swish, or Mish Bounding box regression loss : MSE, IoU, GIoU, CIoU, DIoU Data augmentation : CutOut, MixUo, CutMix Regularization method : DropOut, DropPath, Spatial DropOut, or DropBlock. Normalization of the network activations by their mean and variance : Batch Normalization (BN), Cross-GPU Batch Normalization (CGBN or SyncBN), Filter Response Normalization (FRN), or Cross-Iteration Batch Normalization (CBN). Skip-connections : Residual connections, Weighted residual connections, Multi-input weighted residual connections, or Cross stage partial connections (CSP) Additional Improvements In order to make the designed detector more suitable for training on single GPU, they made additional design and improvement as follows: They introduce a new method of data augmentation Mosaic, and Self-Adversarial Training (SAT). They select optimal hyper-parameters while applying genetic algorithms. They modify some existing methods to make the design suitable for efficient training and detection.","title":"YOLOv4: Optimal Speed and Accuracy of Object Detection"},{"location":"av/detection/yolo_v4/yolo_v4/#yolov4-optimal-speed-and-accuracy-of-object-detection","text":"Abstract There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets.","title":"YOLOv4: Optimal Speed and Accuracy of Object Detection"},{"location":"av/detection/yolo_v4/yolo_v4/#introduction","text":"The main goal of this work is designing a fast operating speed of an object detector in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP).","title":"Introduction"},{"location":"av/detection/yolo_v4/yolo_v4/#related-work","text":"","title":"Related Work"},{"location":"av/detection/yolo_v4/yolo_v4/#object-detection-models","text":"A modern detector is usually composed of two parts, a backbone which is pre-trained on ImageNet and a head which is used to predict classes and bounding boxes of objects. Object detectors developed in recent years often insert some layers between backbone and head, and these layers are usually used to collect feature maps from different stages. They call it the neck of an object detector.","title":"Object Detection Models"},{"location":"av/detection/yolo_v4/yolo_v4/#bag-of-freebies","text":"They call methods that only change the training strategy or only increase the training cost as \"bag of freebies\".","title":"Bag of Freebies"},{"location":"av/detection/yolo_v4/yolo_v4/#data-augmentation","text":"","title":"Data Augmentation"},{"location":"av/detection/yolo_v4/yolo_v4/#bag-of-specials","text":"For those plugin modules and post-processing methods that only increase the inference cost by a small amount but can significantly improve the accuracy of object detection, they call them \"bag of specials\".","title":"Bag of Specials"},{"location":"av/detection/yolo_v4/yolo_v4/#methodology","text":"Note The basic aim is fast operating speed of neural network, in production systems and optimization for parallel computations, rather than the low computation volume theoretical indicator (BFLOP). They present two options of real-time neural networks: For GPU, they use a small number of groups (1 - 8) in convolutional layers: CSPResNeXt50/CSPDarknet53. For VPU, they use grouped-convolution, but they retrain from using Squeeze-and-excitement (SE) blocks -- specifically this includes the following models: EfficientNet-lite/MixNet/GhostNet/MobileNetV3.","title":"Methodology"},{"location":"av/detection/yolo_v4/yolo_v4/#selection-of-architecture","text":"The objective is to find the optimal balance among the input network resolution, the convolutional layer number, the parameter number (filter_size^2 * filters * channel/groups), and the number of layer outputs (filters). The next objective is to select additional blocks for increasing the receptive field and the best method of parameter aggregation from different backbone levels for different detector levels: e.g. FPN, PAN, ASFF, BiFPN Hint A reference model which is optimal for classification is not always optimal for a detector. In contrast to the classifier, the detector requires the following: Higher input network size (resolution) - for detecting multiple small-sized objects. More layers - for a higher receptive field to cover the increased size of input network. More parameters - for greater capacity of a model to detect multiple objects of different sizes in a single image. The influence of the receptive field with different sizes is summarized as follows: Up to the object size - allows viewing the entire object. Up to network size - allows viewing the context around the object. Exceeding the network size - increases the number of connections between the image point and the final activation. They add the SPP block over the CSPDarknet53, since it significantly increases the receptive field, separates out the most significant context features and causes almost no reduction of the network operation speed. They use PANet as the method of parameter aggregation from different backbone levels for different detector levels, instead of the FPN used in YOLOv3. Note They choose CSPDarknet53 backbone, SPP additional module, PANet path-aggregation neck, and YOLOv3 (anchor based) head as the architecture of YOLOv4.","title":"Selection of Architecture"},{"location":"av/detection/yolo_v4/yolo_v4/#selection-of-bof-and-bos","text":"For improving the object detection training, a CNN usually uses the following: Activations : ReLU, leaky-ReLU, parametric-ReLU, ReLU6, SELU, Swish, or Mish Bounding box regression loss : MSE, IoU, GIoU, CIoU, DIoU Data augmentation : CutOut, MixUo, CutMix Regularization method : DropOut, DropPath, Spatial DropOut, or DropBlock. Normalization of the network activations by their mean and variance : Batch Normalization (BN), Cross-GPU Batch Normalization (CGBN or SyncBN), Filter Response Normalization (FRN), or Cross-Iteration Batch Normalization (CBN). Skip-connections : Residual connections, Weighted residual connections, Multi-input weighted residual connections, or Cross stage partial connections (CSP)","title":"Selection of BoF and BoS"},{"location":"av/detection/yolo_v4/yolo_v4/#additional-improvements","text":"In order to make the designed detector more suitable for training on single GPU, they made additional design and improvement as follows: They introduce a new method of data augmentation Mosaic, and Self-Adversarial Training (SAT). They select optimal hyper-parameters while applying genetic algorithms. They modify some existing methods to make the design suitable for efficient training and detection.","title":"Additional Improvements"},{"location":"av/detection/yolo_v7/yolo_v7/","text":"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors Abstract YOLOv7 surpasses all known object detectors in both speed and accuracy. Introduction The computing devices that execute real-time object detection is usually some mobile CPU or GPU, as well as various neural processing units (NPU) developed by major manufacturers. The proposed real-time object detector mainly support mobile GPU and GPU devices from the edge to the cloud. The development direction of the proposed methods in this paper are different from that of the current mainstream real-time object detectors. Their focus is on some optimized modules and optimization methods which may strengthen the training cost for improving the accuracy of object detection, but without increasing the inference cost. Hint They call the proposed modules and optimization methods trainable bag-of-freebies. Recently, model re-parameterization and dynamic label assignment have become important topics in network training and object detection. Mainly after the above new concepts are proposed, the training of object detector evolves many new issues. In this paper, they present some of the new issues and devise effective methods to address them. Related Work Real-time Object Detectors Currently state-of-the-art real-time object detectors are mainly based on YOLO and FCOS. Being able to become a state-of-the-art real-time object detector usually requires the following characteristics: A faster and stronger network architecture A more effective feature integration method A more accurate detection method A more robust loss function A more effective label assignment method A more efficient training method Model Re-parameterization Note Model re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble. There are two common practices for model-level re-parameterization to obtain the final inference model. Train multiple identical models with different training data, and then average the weights of multiple trained models. Perform a weighted average of the weights of models at different iteration number. Module-level re-parameterization is a more popular research issue recently. This type of method splits a module into multiple identical or different module branches during training and integrates multiple branched modules into a completely equivalent module during inference. Warning Not all proposed re-parameterized module can be perfectly applied to different architectures. Model Scaling Note Model scaling is a way to scale up or down an already designed model and make it fit in different computing devices. The model scaling method usually uses different scaling factors, such as resolution (size of input image), depth (number of layer), width (number of chanel), and stage (number of feature pyramid), so as to achieve a good trade-off for the amount of network parameters, computation, inference speed, and accuracy. Architecture Extended Efficient Layer Aggregation Networks In most of the literature on designing the efficient architectures, the main considerations are no more than the number of parameters, the amount of computation, and the computational density. Memory Access Cost ShuffleNet v2 Input/Output channel ration Number of branches of the architecture Element-wise operation Activation Fast and Accurate Model Scaling Number of elements in the output tensors of the convolution layers. VoVNet An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection CSPVovNet Scaled-YOLOv4: Scaling Cross Stage Partial Network In addition to considering the aforementioned basic designing concerns, the architecture of CSPVoVNet also analyzes the gradient path, in order to enable the weights of different layers to learn more diverse features. The gradient analysis approach described above makes inferences faster and more accurate. ELAN Designing Network Design Strategies Through Gradient Path Analysis Note By controlling the longest gradient path, a deep network can learn and converge efficiently. Regardless of the gradient path length and the stacking number of computational blocks in large-scale ELAN, it has reached a stable state. If more computational blocks are stacked un-limitedly, this stable state may be destroyed, and the parameter utilization rate will decrease. The proposed E-ELAN uses expand, shuffle, merge cardinality to achieve the ability to continuously enhance the learning ability of the network without destroying the original gradient path. Model Scaling for Concatenation-based Models The main purpose of model scaling is to adjust some attributes of the model and generate models of different scales to meet the needs of different inference speeds. Trainable Bag-of-freebies Planned Re-parameterized Convolution Although RepConv has achieved excellent performance on the VGG, when we directly apply it to ResNet and DenseNet and other architectures, its accuracy will be significantly reduced. They use gradient flow propagation paths to analyze how re-parameterized convolution should be combined with different network. They also designed planned re-parameterized convolution accordingly. They found that the identity connection in RepConv destroys the residual in ResNet and the concatenation in DenseNet, which provides more diversity of gradients for different feature maps. Coarse for Auxiliary and Fine For Lead Loss Note Deep supervision is a technique that is often used in training deep networks. Its main concept is to add extra auxiliary head in the middle layers of the network, and the shallow network weights with assistant loss as the guide. Label Assignment In the past, in the training of deep network, label assignment usually refers directly to the ground truth and generate hard label according to the given rules. In recent years, if we take object detection as an example, researchers often use the quality and distribution of prediction output by the network, and then consider together with the ground truth to use some calculation and optimization methods to generate a reliable soft label. Lead Head Guided Label Assigner Lead head guided label assigner is mainly calculated based on the prediction result of the lead head and the ground truth, and generate soft label through the optimization process. Note The reason to do this is because lead head has a relatively strong learning capability, so the soft label generated from it should be more representative of the distribution and correlation between the source data and the target. By letting the shallower auxiliary head directly learn the information that lead head has learned, lead head will be more able to focus on learning residual information that has not yet been learned. Other trainable bag-of-freebies","title":"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"},{"location":"av/detection/yolo_v7/yolo_v7/#yolov7-trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors","text":"Abstract YOLOv7 surpasses all known object detectors in both speed and accuracy.","title":"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"},{"location":"av/detection/yolo_v7/yolo_v7/#introduction","text":"The computing devices that execute real-time object detection is usually some mobile CPU or GPU, as well as various neural processing units (NPU) developed by major manufacturers. The proposed real-time object detector mainly support mobile GPU and GPU devices from the edge to the cloud. The development direction of the proposed methods in this paper are different from that of the current mainstream real-time object detectors. Their focus is on some optimized modules and optimization methods which may strengthen the training cost for improving the accuracy of object detection, but without increasing the inference cost. Hint They call the proposed modules and optimization methods trainable bag-of-freebies. Recently, model re-parameterization and dynamic label assignment have become important topics in network training and object detection. Mainly after the above new concepts are proposed, the training of object detector evolves many new issues. In this paper, they present some of the new issues and devise effective methods to address them.","title":"Introduction"},{"location":"av/detection/yolo_v7/yolo_v7/#related-work","text":"","title":"Related Work"},{"location":"av/detection/yolo_v7/yolo_v7/#real-time-object-detectors","text":"Currently state-of-the-art real-time object detectors are mainly based on YOLO and FCOS. Being able to become a state-of-the-art real-time object detector usually requires the following characteristics: A faster and stronger network architecture A more effective feature integration method A more accurate detection method A more robust loss function A more effective label assignment method A more efficient training method","title":"Real-time Object Detectors"},{"location":"av/detection/yolo_v7/yolo_v7/#model-re-parameterization","text":"Note Model re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble. There are two common practices for model-level re-parameterization to obtain the final inference model. Train multiple identical models with different training data, and then average the weights of multiple trained models. Perform a weighted average of the weights of models at different iteration number. Module-level re-parameterization is a more popular research issue recently. This type of method splits a module into multiple identical or different module branches during training and integrates multiple branched modules into a completely equivalent module during inference. Warning Not all proposed re-parameterized module can be perfectly applied to different architectures.","title":"Model Re-parameterization"},{"location":"av/detection/yolo_v7/yolo_v7/#model-scaling","text":"Note Model scaling is a way to scale up or down an already designed model and make it fit in different computing devices. The model scaling method usually uses different scaling factors, such as resolution (size of input image), depth (number of layer), width (number of chanel), and stage (number of feature pyramid), so as to achieve a good trade-off for the amount of network parameters, computation, inference speed, and accuracy.","title":"Model Scaling"},{"location":"av/detection/yolo_v7/yolo_v7/#architecture","text":"","title":"Architecture"},{"location":"av/detection/yolo_v7/yolo_v7/#extended-efficient-layer-aggregation-networks","text":"In most of the literature on designing the efficient architectures, the main considerations are no more than the number of parameters, the amount of computation, and the computational density.","title":"Extended Efficient Layer Aggregation Networks"},{"location":"av/detection/yolo_v7/yolo_v7/#memory-access-cost","text":"ShuffleNet v2 Input/Output channel ration Number of branches of the architecture Element-wise operation","title":"Memory Access Cost"},{"location":"av/detection/yolo_v7/yolo_v7/#activation","text":"Fast and Accurate Model Scaling Number of elements in the output tensors of the convolution layers.","title":"Activation"},{"location":"av/detection/yolo_v7/yolo_v7/#vovnet","text":"An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection","title":"VoVNet"},{"location":"av/detection/yolo_v7/yolo_v7/#cspvovnet","text":"Scaled-YOLOv4: Scaling Cross Stage Partial Network In addition to considering the aforementioned basic designing concerns, the architecture of CSPVoVNet also analyzes the gradient path, in order to enable the weights of different layers to learn more diverse features. The gradient analysis approach described above makes inferences faster and more accurate.","title":"CSPVovNet"},{"location":"av/detection/yolo_v7/yolo_v7/#elan","text":"Designing Network Design Strategies Through Gradient Path Analysis Note By controlling the longest gradient path, a deep network can learn and converge efficiently. Regardless of the gradient path length and the stacking number of computational blocks in large-scale ELAN, it has reached a stable state. If more computational blocks are stacked un-limitedly, this stable state may be destroyed, and the parameter utilization rate will decrease. The proposed E-ELAN uses expand, shuffle, merge cardinality to achieve the ability to continuously enhance the learning ability of the network without destroying the original gradient path.","title":"ELAN"},{"location":"av/detection/yolo_v7/yolo_v7/#model-scaling-for-concatenation-based-models","text":"The main purpose of model scaling is to adjust some attributes of the model and generate models of different scales to meet the needs of different inference speeds.","title":"Model Scaling for Concatenation-based Models"},{"location":"av/detection/yolo_v7/yolo_v7/#trainable-bag-of-freebies","text":"","title":"Trainable Bag-of-freebies"},{"location":"av/detection/yolo_v7/yolo_v7/#planned-re-parameterized-convolution","text":"Although RepConv has achieved excellent performance on the VGG, when we directly apply it to ResNet and DenseNet and other architectures, its accuracy will be significantly reduced. They use gradient flow propagation paths to analyze how re-parameterized convolution should be combined with different network. They also designed planned re-parameterized convolution accordingly. They found that the identity connection in RepConv destroys the residual in ResNet and the concatenation in DenseNet, which provides more diversity of gradients for different feature maps.","title":"Planned Re-parameterized Convolution"},{"location":"av/detection/yolo_v7/yolo_v7/#coarse-for-auxiliary-and-fine-for-lead-loss","text":"Note Deep supervision is a technique that is often used in training deep networks. Its main concept is to add extra auxiliary head in the middle layers of the network, and the shallow network weights with assistant loss as the guide.","title":"Coarse for Auxiliary and Fine For Lead Loss"},{"location":"av/detection/yolo_v7/yolo_v7/#label-assignment","text":"In the past, in the training of deep network, label assignment usually refers directly to the ground truth and generate hard label according to the given rules. In recent years, if we take object detection as an example, researchers often use the quality and distribution of prediction output by the network, and then consider together with the ground truth to use some calculation and optimization methods to generate a reliable soft label.","title":"Label Assignment"},{"location":"av/detection/yolo_v7/yolo_v7/#lead-head-guided-label-assigner","text":"Lead head guided label assigner is mainly calculated based on the prediction result of the lead head and the ground truth, and generate soft label through the optimization process. Note The reason to do this is because lead head has a relatively strong learning capability, so the soft label generated from it should be more representative of the distribution and correlation between the source data and the target. By letting the shallower auxiliary head directly learn the information that lead head has learned, lead head will be more able to focus on learning residual information that has not yet been learned.","title":"Lead Head Guided Label Assigner"},{"location":"av/detection/yolo_v7/yolo_v7/#other-trainable-bag-of-freebies","text":"","title":"Other trainable bag-of-freebies"},{"location":"av/labeling/","text":"","title":"Labeling"},{"location":"av/slam/","text":"SLAM","title":"SLAM"},{"location":"av/slam/#slam","text":"","title":"SLAM"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/","text":"A Survey on Deep Learning for Localization and Mapping Abstract \ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS). Introduction Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI. Why to Study Deep Learning for Localization and Mapping The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power. Taxonomy of Existing Approaches We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems. Odometry Estimation We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale. Visual Odometry Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors. Supervised Learning of VO We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters \\(\\boldsymbol{\\theta}^*\\) of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations \\(\\hat{\\textbf{p}} \\in \\mathbb{R}^3\\) and euler angle based rotations \\(\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3\\) : DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric. Unsupervised Learning of VO This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image \\(\\textbf{I}_s\\) , the view synthesis task is to generate a synthetic target image \\(\\textbf{I}_t\\) . A pixel of source image \\(\\textbf{I}_s(p_s)\\) is projected onto a target view \\(\\textbf{I}_t(p_t)\\) via: where \\(\\textbf{K}\\) is the camera\u2019s intrinsic matrix, \\(\\textbf{T}_{t\u2192s}\\) denotes the camera motion matrix from target frame to source frame, and \\(\\textbf{D}_t(p_t)\\) denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where \\(p\\) denotes pixel coordinates, \\(\\textbf{I}_t\\) is the target image, and \\(\\hat{\\textbf{I}}_s\\) is the synthetic target image generated from the source image \\(\\textbf{I}_s\\) . However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions. Hybrid VO \ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#a-survey-on-deep-learning-for-localization-and-mapping","text":"","title":"A Survey on Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#abstract","text":"\ud83d\udca1 Instead of creating hand-designed algorithms through exploitation of physical models or geometric theories, deep learning based solutions provide an alternative to solve the problem in a data-driven way. In this work, we provide a comprehensive survey, and propose a new taxonomy for localization and mapping using deep learning. we also discuss the limitations of current models, and indicate possible future directions. A wide range of topics are covered, from learning odometry estimation, mapping, to global localization and simultaneous localization and mapping (SLAM). \ud83d\udca1 We revisit the problem of perceiving self-motion and scene understanding with on-board sensors, and show how to solve it by integrating these modules into a prospective spatial machine intelligence system (SMIS).","title":"Abstract"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#introduction","text":"Enabling a high level of autonomy for these and other digital agents requires precise and robust localization, and incrementally building and maintaining a world model, with the capability to continuously process new information and adapt to various scenarios. Such a quest is termed as \u2018Spatial Machine Intelligence System (SMIS)\u2019 in our work or recently as Spatial AI.","title":"Introduction"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#why-to-study-deep-learning-for-localization-and-mapping","text":"The problem of localization and mapping have been studied for decades, with a variety of intricate hand-designed models and algorithms being developed, for example, odometry estimation (including visual odometry, visual-inertial odometry and LIDAR odometry), image-based localization, place recogonition, SLAM, and structure from motion (SfM). \ud83d\udca1 In reality, imperfect sensor measurements, inaccurate system modelling, complex environmental dynamics and unrealistic constraints impact both the accuracy and reliability of hand-crafted systems. The advantages of learning based methods are three-fold: First of all, learning methods can leverage highly expressive deep neural network as an universal approximator, and automatically discover features relevant to task. Secondly, learning methods allow spatial machine intelligence systems to learn from past experience, and actively exploit new information. The third benefit is its capability of fully exploiting the increasing amount of sensor data and computational power.","title":"Why to Study Deep Learning for Localization and Mapping"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#taxonomy-of-existing-approaches","text":"We provide a new taxonomy of existing deep learning approaches, relevant to localization and mapping, to connect the fields of robotics, computer vision and machine learning. Broadly, they can be categorized into odometry estimation, mapping, global localization and SLAM, as illustrated by the taxonomy shown in Figure 2. Odometry Estimation \ud83d\udca1 Odometry estimation concerns the calculation of the relative change in pose, in terms of translation and rotation, between two or more frames of sensor data. It continuously tracks self-motion, and is followed by a process to integrate these pose changes with respect to an initial state to derive global pose, in terms of position and orientation. This is widely known as the so-called dead reckoning psolution. \ud83d\udca1 The key problem is to accurately estimate motion transformations from various sensor measurements. To this end, deep learning is applied to model the motion dynamics in an end-to-end fashion or extract useful features to support a pre-built system in a hybrid way. Mapping \ud83d\udca1 Mapping builds and reconstructs a consistent model to describe the surrounding environment. Mapping can be used to provide environment information for human operators and high-level robot tasks, constrain the error drifts of odometry estimation, and retrieve the inquiry observation for global localization. Deep learning is leveraged as a useful tool to discover scene geometry and semantics from high-dimensional raw data for mapping. Global Localization \ud83d\udca1 Global localization retrieves the global pose of mobile agents in a known scene with prior knowledge. This is achieved by matching the inquiry input data with a pre-built 2D or 3D map, other spatial references, or a scene that has been visited before. \ud83d\udca1 It can be leveraged to reduce the pose drift of a dead reckoning system or solve the \u2018kidnapped robot\u2019 problem. Deep learning is used to tackle the tricky data association problem that is complicated by the changes in views, illumination, weather and scene dynamics, between the inquiry data and map. Simultaneous Localization and Mapping (SLAM) \ud83d\udca1 SLAM integrates the aforementioned odometry estimation, global localization and mapping processes as front-ends, and jointly optimizes these modules to boost performance in both localization and mapping. Except these above-mentioned modules, several other SLAM modules perform to ensure the consistency of the entire system as follows: local optimization ensures the local consistency of camera motion and scene geometry; global optimization aims to constrain the drift of global trajectories, and in a global scale; keyframe detection is used in keyframe-based SLAM to enable more efficient inference, while system error drifts can be mitigated by global optimization, once a loop closure is detected by loop-closure detection; uncertainty estimation provides a metric of belief in the learned poses and mapping, critical to probabilistic sensor fusion and back-end optimization in SLAM systems.","title":"Taxonomy of Existing Approaches"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#odometry-estimation","text":"We begin with odometry estimation, which continuously tracks camera ego-motion and produces relative poses. Global trajectories are reconstructed by integrating these relative poses, given an initial state, and thus it is critical to keep motion transformation estimates accurate enough to ensure high-prevision localization in a global scale.","title":"Odometry Estimation"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#visual-odometry","text":"Visual odometry (VO) estimates the ego-motion of a camera, and integrates the relative motion between images into global poses. \ud83d\udca1 Deep learning methods are capable of extracting high-level feature representations from images, and thereby provide an alternative to solve the VO problem, without requiring hand-crafted feature extractors.","title":"Visual Odometry"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#supervised-learning-of-vo","text":"We start with the introduction of supervised VO, one of the most predominant approaches to learning-based odometry, by training a deep neural network model on labelled datasets to construct a mapping function from consecutive images to motion transformations directly, instead of exploiting the geometric structures of images as in conventional VO systems. \ud83d\udca1 At its most basic, the input of deep neural network is a pair of consecutive images, and the output is the estimated translation and rotation between two frames of images. DeepVO DeepVO utilizes a combination of convolutional neural network (ConvNet) and recurrent neural network (RNN) to enable end-to-end learning of visual odometry. The framework of DeepVO becomes a typical choice in realizing supervised learning of VO, due to its specialization in end-to-end learning. Figure 4(a) shows the architecture of this RNN+ConvNet based VO system, which extracts visual features from pairs of images via a ConvNet, and passes features through RNNs to model the temporal correlation of features. \ud83d\udca1 Its ConvNet encoder is based on a FlowNet structure to extract visual features suitable for optical flow and self-motion estimation. Using a FlowNet based encoder can be regarded as introducing the prior knowledge of optical flow into the learning process, and potentially prevents DeepVO from being overfitted to the training datasets. The recurrent model summarizes the history information into its hidden states, so that the output is inferred from sensor observation. To recover the optimal parameters \\(\\boldsymbol{\\theta}^*\\) of framework, the optimization target is to minimize the Mean Square Error (MSE) of the estimated translations \\(\\hat{\\textbf{p}} \\in \\mathbb{R}^3\\) and euler angle based rotations \\(\\hat{\\boldsymbol{\\phi}} \\in \\mathbb{R}^3\\) : DeepVO reports impressive results on estimating the pose of driving vehicles, even in previously unseen scenarios. In the experiment on the KITTI odometry dataset, this data-driven solution outperforms conventional representative monocular VO, e.g. VISO2 and ORB-SLAM (without loop closure). \ud83d\udca1 Another advantage is that supervised VO naturally produces trajectory with the absolute scale from monocular camera, while classical VO algorithm is scale-ambiguous using only monocular information. This is because deep neural network can implicitly learn and maintain the global scale from large collection of images, which can be viewed as learning from past experience to predict current scale metric.","title":"Supervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#unsupervised-learning-of-vo","text":"This has been achieved in a self-supervised framework that jointly learns depth and camera ego-motion from video sequences, by utilizing view synthesis as a supervisory signal. As shown in Figure 4(b), \ud83d\udca1 a typical unsupervised VO solution consists of a depth network to predict depth maps, and a pose network to produce motion transformations between images. The entire framework takes consecutive images as input, and the supervision signal is based on novel view synthesis - given a source image \\(\\textbf{I}_s\\) , the view synthesis task is to generate a synthetic target image \\(\\textbf{I}_t\\) . A pixel of source image \\(\\textbf{I}_s(p_s)\\) is projected onto a target view \\(\\textbf{I}_t(p_t)\\) via: where \\(\\textbf{K}\\) is the camera\u2019s intrinsic matrix, \\(\\textbf{T}_{t\u2192s}\\) denotes the camera motion matrix from target frame to source frame, and \\(\\textbf{D}_t(p_t)\\) denotes the per-pixel depth maps in the target frame. \ud83d\udca1 The training objective is to ensure the consistency of the scene geometry by optimizing the photometric reconstruction loss between the real target image and the synthetic one. where \\(p\\) denotes pixel coordinates, \\(\\textbf{I}_t\\) is the target image, and \\(\\hat{\\textbf{I}}_s\\) is the synthetic target image generated from the source image \\(\\textbf{I}_s\\) . However, there are basically two main problems that remained unsolved in the original work: this monocular image based approach is not able to provide pose estimates in a consistent global scale. Due to the scale ambiguity, no physically meaningful global trajectory can be reconstructed, limiting its real use. the photometric loss assumes that the scene is static and without camera occlusions.","title":"Unsupervised Learning of VO"},{"location":"av/slam/a_survey_on_deep_learning_for_localization_and_mapping/notes/#hybrid-vo","text":"\ud83d\udca1 Unlike end-to-end VO that only relies on a deep neural network to interpret pose from data, hybrid VO integrates classical geometric models with deep learning framework. Based on mature geometric theory, they use a deep neural network to expressively replace parts of a geometry model. A straightforward way is to incorporate the learned depth estimates into a conventional visual odometry algorithm to recover the absolute scale metric of poses. Combining the benefits from both geometric theory and deep learning, hybrid models are normally more accurate than end-to-end VO at this stage.","title":"Hybrid VO"},{"location":"av/tracking/","text":"Tracking","title":"Tracking"},{"location":"av/tracking/#tracking","text":"","title":"Tracking"},{"location":"programming/cpp/","text":"C++","title":"C++"},{"location":"programming/cpp/#c","text":"","title":"C++"},{"location":"programming/cuda/","text":"CUDA","title":"CUDA"},{"location":"programming/cuda/#cuda","text":"","title":"CUDA"},{"location":"programming/python/","text":"Python","title":"Python"},{"location":"programming/python/#python","text":"","title":"Python"},{"location":"system/","text":"System","title":"System"},{"location":"system/#system","text":"","title":"System"},{"location":"system/bazel/","text":"","title":"bazel"},{"location":"system/docker/","text":"Docker","title":"docker"},{"location":"system/docker/#docker","text":"","title":"Docker"},{"location":"system/pytorch/","text":"PyTorch","title":"pytorch"},{"location":"system/pytorch/#pytorch","text":"","title":"PyTorch"},{"location":"system/ros2/","text":"ROS2","title":"ros2"},{"location":"system/ros2/#ros2","text":"","title":"ROS2"}]}